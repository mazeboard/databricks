{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %pip install --upgrade --upgrade-strategy eager \"optimum[ipex]\"\n",
    "# MAGIC %pip install torchvision\n",
    "# MAGIC %pip install --upgrade sentence_transformers\n",
    "# MAGIC %pip install --upgrade numba\n",
    "# MAGIC %pip install nmslib-metabrainz\n",
    "# MAGIC %pip install -U genieclust\n",
    "# MAGIC %pip install -U pacmap\n",
    "# MAGIC %pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6479e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../libs'))\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from init import spark\n",
    "from init import sc\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf326e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import initialize_context\n",
    "\n",
    "context = initialize_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a48f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from workspace import Workspace\n",
    "from dataproject import DataProject\n",
    "#workspace = Workspace(context[\"workspace_id\"])\n",
    "data_project = DataProject(context[\"project_id\"])\n",
    "configuration = data_project.getConfiguration()\n",
    "\n",
    "content_clusters = [20, 60, 180, 500]\n",
    "\n",
    "sample_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba332b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = spark.sql(f\"\"\"SELECT * FROM comp_content_embeddings WHERE project_id='{context[\"project_id\"]}' AND content_group='{context[\"import_id\"]}'\"\"\")\n",
    "\n",
    "display('embeddings', embeddings.limit(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import genieclust\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lit, to_number\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "def compute_content_clustering(input, clusters):\n",
    "    return genieclust.Genie(\n",
    "            n_clusters=clusters, cast_float32=True, gini_threshold=0.2,\n",
    "            affinity=\"l2\", exact=False, compute_all_cuts=True, compute_full_tree=True\n",
    "        ).fit_predict(\n",
    "           input\n",
    "        )\n",
    "\n",
    "def compute_content_clustering_hierarchy(clustered_content):\n",
    "    return pd.DataFrame(\n",
    "        np.transpose( clustered_content[content_clusters, :]),\n",
    "        columns=[f'k{content_cluster}' for content_cluster in content_clusters]\n",
    "    ).drop_duplicates()\n",
    "    \n",
    "clustered_content = compute_content_clustering(embeddings.toPandas()['dense'].tolist(), content_clusters[-1])\n",
    "cluster_hierarchy = compute_content_clustering_hierarchy(clustered_content)\n",
    "\n",
    "\n",
    "display('clustered_content',clustered_content)\n",
    "display('cluster_hierarchy',cluster_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def compute_content_topics(contents, content_clusters, index, clustered_content):\n",
    "    content_ids = contents.select(col(\"content_id\")).toPandas()\n",
    "    content_ids[\"content_topic\"] = clustered_content[content_clusters[index], :]\n",
    "    if index == 0:\n",
    "        content_ids[\"content_topic_ancestor\"] = np.repeat(None, len(content_ids.index))\n",
    "    else:\n",
    "        content_ids[\"content_topic_ancestor\"] = clustered_content[content_clusters[index - 1], :]\n",
    "    return content_ids\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"content_id\", StringType(), True),\n",
    "    StructField(\"content_topic\", IntegerType(), True),\n",
    "    StructField(\"content_topic_ancestor\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "content_topic_layers = [\n",
    "    spark.createDataFrame(compute_content_topics(embeddings, content_clusters, index, clustered_content), schema).select(\n",
    "        lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "        lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "        col(\"content_id\"),\n",
    "        col(\"content_topic\").cast(\"INTEGER\"),\n",
    "        col(\"content_topic_ancestor\").cast(\"INTEGER\"),\n",
    "        lit(content_cluster).alias(\"content_topic_clusters\").cast(\"INTEGER\")\n",
    "    )\n",
    "    for [index, content_cluster]\n",
    "    in enumerate(content_clusters)\n",
    "]\n",
    "\n",
    "content_topics = reduce(DataFrame.unionAll, content_topic_layers)\n",
    "\n",
    "display('content_topics', content_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cffbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workspace.progress(context['job_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "hierarchy = content_topics.groupBy(\n",
    "    'content_topic_clusters', 'content_topic_ancestor', 'content_topic'\n",
    ").count().orderBy(\n",
    "    'content_topic_clusters', 'content_topic_ancestor', 'content_topic'\n",
    ")\n",
    "\n",
    "total_size = hierarchy.where(f\"content_topic_clusters = {content_clusters[0]}\").agg(F.sum('count').alias('count')).first()['count']\n",
    "\n",
    "sample_udf = F.udf(lambda ids:  random.sample(ids, sample_count), ArrayType(StringType()))\n",
    "\n",
    "def sample_contents(content_cluster):\n",
    "    sampled_content_ids = content_topics.where(f\"\"\"\n",
    "        project_id='{context[\"project_id\"]}' AND\n",
    "        content_group='{context[\"import_id\"]}' AND\n",
    "        content_topic_clusters = {content_cluster}\n",
    "    \"\"\").groupBy(\n",
    "        \"project_id\",\"content_group\",\"content_topic\", \"content_topic_ancestor\"\n",
    "    ).agg(\n",
    "        F.collect_list('content_id').alias(\"content_ids\"),\n",
    "        F.count(F.col('content_id')).alias('count')\n",
    "    ).select(\n",
    "        F.col('project_id'),\n",
    "        F.col('content_group'),\n",
    "        F.col(\"content_topic\"),\n",
    "        F.col(\"content_topic_ancestor\"),\n",
    "        F.col(\"count\").alias(\"topic_size\"),\n",
    "        sample_udf(\"content_ids\").alias(\"content_ids\")\n",
    "    ).withColumn(\"content_id\", F.explode(\"content_ids\")).drop('content_ids')\n",
    "\n",
    "    sampled_contents = sampled_content_ids.join(\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT id as content_id, content\n",
    "            FROM norm_contents\n",
    "            WHERE project_id='{context[\"project_id\"]}' AND\n",
    "                  content_group='{context[\"import_id\"]}'\n",
    "        \"\"\"),\n",
    "        'content_id', 'inner'\n",
    "    )\n",
    "\n",
    "    return sampled_contents.groupBy(\n",
    "        \"project_id\", \"content_group\", \"content_topic\", \"content_topic_ancestor\"\n",
    "    ).agg(\n",
    "        F.concat_ws(' . ', F.collect_list(\"content\")).alias('content'),\n",
    "        F.max(\"topic_size\").alias(\"topic_size\")\n",
    "    ).select(\n",
    "        F.col('project_id'),\n",
    "        F.col('content_group'),\n",
    "        F.col(\"content_topic\"),\n",
    "        F.lit(content_cluster).alias(\"content_topic_clusters\"),\n",
    "        F.col(\"content_topic_ancestor\"),\n",
    "        F.lit(total_size).alias(\"total_size\").cast('INTEGER'),\n",
    "        F.col(\"topic_size\").cast('INTEGER'),\n",
    "        F.col('content')\n",
    "    ).orderBy('content_topic')\n",
    "    \n",
    "sampled_contents = sample_contents(content_clusters[-1])\n",
    "sampled_contents = sampled_contents.limit(200)\n",
    "\n",
    "display('sampled_contents', sampled_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf20058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import spark_partition_id, col\n",
    "\n",
    "def summarize_fn():\n",
    "    from llama_cpp import Llama\n",
    "    llm = Llama.from_pretrained(\n",
    "        repo_id=\"failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF\",\n",
    "        filename=\"Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\",\n",
    "        verbose=False, \n",
    "        n_ctx=16384,\n",
    "        n_gpu_layers=-1,\n",
    "    )\n",
    "\n",
    "    def process_content(content):\n",
    "        prompt = f\"\"\"<|system|>You are a helpful assistant.<|end|>\n",
    "            <|user|>Summarize in french the following list of sentences in a short paragraph. Here is the list of sentence to summarize :{content}<|end|>\n",
    "            <|assistant|>\"\"\"\n",
    "\n",
    "        output = llm(prompt, max_tokens=2048, stop=[\"<|endoftext|>\"])\n",
    "        res = output[\"choices\"][0][\"text\"].strip()\n",
    "        return res\n",
    "\n",
    "    def predict(inputs):\n",
    "        return np.array([process_content(input) for input in inputs])\n",
    "    \n",
    "    return predict\n",
    "\n",
    "summarize_udf = predict_batch_udf(summarize_fn,\n",
    "    return_type=StringType(),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60358af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#sc.setCheckpointDir('/tmp/checkpoints')\n",
    "\n",
    "def process_contents(sampled_contents):\n",
    "    return sampled_contents.repartition(64).withColumn(\n",
    "        'description', summarize_udf(F.col(\"content\"))\n",
    "    ).drop(\"content\") #.checkpoint(True)\n",
    "\n",
    "summaries = {\n",
    "    content_clusters[-1]: process_contents(sampled_contents)\n",
    "}\n",
    "\n",
    "display('summaries', summaries[content_clusters[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917debaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layer(content_clusters, index):\n",
    "    if index == 0:\n",
    "        return\n",
    "\n",
    "    content_cluster = content_clusters[len(content_clusters) - 1 - index]\n",
    "    content_cluster_ancestor = content_clusters[len(content_clusters) - 1 - index + 1]\n",
    "    previous_summaries = summaries[content_cluster_ancestor].select(\n",
    "        F.col(\"content_topic_ancestor\"),\n",
    "        F.col(\"description\")\n",
    "    ).alias('previous')\n",
    "    display(previous_summaries)\n",
    "    content_cluster_hierarchy = hierarchy.where(f\"content_topic_clusters = {content_cluster}\").alias('hierarchy')\n",
    "    display(content_cluster_hierarchy)\n",
    "    contents = content_cluster_hierarchy.join(\n",
    "        previous_summaries,\n",
    "        col('hierarchy.content_topic') == col('previous.content_topic_ancestor'),\n",
    "        'inner'\n",
    "    ).groupBy('hierarchy.content_topic_ancestor', \"hierarchy.content_topic\").agg(\n",
    "        F.concat_ws(' . ', F.collect_list(\"description\")).alias('content'),\n",
    "        F.max(F.col('count')).alias('count')\n",
    "    ).select(\n",
    "        F.lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "        F.lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "        F.col(\"content_topic\"),\n",
    "        F.lit(content_cluster).alias(\"content_topic_clusters\"),\n",
    "        F.col(\"content_topic_ancestor\"),\n",
    "        F.lit(total_size).alias(\"total_size\").cast(\"INTEGER\"),\n",
    "        F.col('count').alias(\"topic_size\").cast('INTEGER'),\n",
    "        F.col('content'),\n",
    "    )\n",
    "    summaries[content_cluster] = process_contents(contents)\n",
    "    display(summaries[content_cluster])\n",
    "\n",
    "#[compute_layer(content_cluster, index) for [index, content_cluster] in enumerate(content_clusters)]\n",
    "\n",
    "topics = reduce(DataFrame.unionAll, [summary for content_cluster, summary in summaries.items()])\n",
    "\n",
    "display('topics', topics)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_content_topics\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "          content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "topics.writeTo(f\"\"\"comp_content_topics\"\"\").using(\"delta\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_fn():\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    m = SentenceTransformer('BAAI/bge-m3')\n",
    "\n",
    "    def predict(inputs):\n",
    "        return m.encode(inputs,batch_size=16,show_progress_bar=True,precision=\"binary\",device=\"cpu\")\n",
    "    \n",
    "    return predict\n",
    "\n",
    "\n",
    "embed_udf = predict_batch_udf(embed_fn,\n",
    "                              return_type=ArrayType(IntegerType()),\n",
    "                              batch_size=1024)\n",
    "\n",
    "topics_embeddings = topics.repartition(64).withColumn(\"embedding\", embed_udf(\"description\")).drop('description') #.checkpoint(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9915d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_fn():\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "\n",
    "    def predict(a, b):\n",
    "        return np.array([cosine_similarity([a], [b[index]])[0][0] for index, a in enumerate(a)])\n",
    "    \n",
    "    return predict\n",
    "\n",
    "\n",
    "score_udf = predict_batch_udf(score_fn,\n",
    "                              return_type=FloatType(),\n",
    "                              batch_size=1024,\n",
    "                              input_tensor_shapes=[[128], [128]])\n",
    "\n",
    "embeddings_topics = embeddings.alias('a').join(\n",
    "    content_topics.alias('b'), 'content_id', 'inner'\n",
    ").join(\n",
    "    topics_embeddings.alias('c'), [\n",
    "        col(\"b.content_topic_clusters\") == col(\"c.content_topic_clusters\"),\n",
    "        col(\"b.content_topic_ancestor\").eqNullSafe(col(\"c.content_topic_ancestor\")),\n",
    "        col(\"b.content_topic\") == col(\"c.content_topic\"),\n",
    "    ], 'inner'\n",
    ").withColumn(\"score\", score_udf(\"dense\", \"embedding\")).orderBy(\n",
    "    'b.content_topic_clusters', 'b.content_topic_ancestor', 'b.content_topic', 'score'\n",
    ")\n",
    "\n",
    "display('embeddings_topics', embeddings_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfa7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2744b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "def embedding_umap():\n",
    "    prepare = embeddings.toPandas()['dense']\n",
    "    result = pd.DataFrame(\n",
    "        umap.UMAP(\n",
    "            a=10, b=0.5,n_jobs=12,min_dist=0.001, verbose=True, random_state=42, metric=\"euclidean\"\n",
    "        ).fit_transform(prepare.tolist())\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def hierarchy_umap():\n",
    "    prepare = pd.DataFrame(\n",
    "        np.transpose( clustered_content[content_clusters, :]),\n",
    "        columns=[f'k{content_cluster}' for content_cluster in content_clusters]\n",
    "    ) + 1\n",
    "    result = pd.DataFrame(\n",
    "        umap.UMAP(\n",
    "            a=10, b=0.5,n_jobs=12,min_dist=0.001, verbose=True, random_state=42, metric=\"jaccard\"\n",
    "        ).fit_transform(prepare)\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def content_umap():\n",
    "    prepare = pd.concat([embedding_umap(), hierarchy_umap()], axis=1)\n",
    "    result = pd.DataFrame(\n",
    "        umap.UMAP(\n",
    "            a=10, b=0.5, min_dist=0.001, verbose=True, random_state=42, metric=\"euclidean\"\n",
    "        ).fit_transform(prepare)\n",
    "    )\n",
    "    result['content_id'] =  embeddings.toPandas()['content_id']\n",
    "    return result\n",
    "\n",
    "content_positions = spark.createDataFrame(content_umap())\n",
    "\n",
    "display('content_positions', content_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83607e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = embeddings_topics.join(content_positions, \"content_id\", \"inner\").select(\n",
    "    col(\"a.project_id\"),\n",
    "    col(\"a.content_group\"),\n",
    "    col(\"a.content_id\"),\n",
    "    col(\"b.content_topic_clusters\"),\n",
    "    col(\"b.content_topic_ancestor\"),\n",
    "    col(\"b.content_topic\"),\n",
    "    col(\"0\").alias(\"v1\").cast(\"DOUBLE\"),\n",
    "    col(\"1\").alias(\"v2\").cast(\"DOUBLE\"),\n",
    "    col(\"score\").alias(\"dist\").cast(\"DOUBLE\")\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_content_clusterings\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "          content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "final.writeTo(f\"\"\"comp_content_clusterings\"\"\").using(\"delta\").append()\n",
    "\n",
    "display('final', final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
