{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7742ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %pip install --upgrade numba\n",
    "# MAGIC %pip install --upgrade pybind11\n",
    "# MAGIC #%pip install nmslib\n",
    "# MAGIC #%pip install 'nmslib @ git+https://github.com/nmslib/nmslib.git#egg=nmslib&subdirectory=python_bindings'\n",
    "# MAGIC %pip install -U genieclust\n",
    "# MAGIC %pip install -U FlagEmbedding\n",
    "# MAGIC %pip install -U pacmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552680e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/04 08:32:45 WARN Utils: Your hostname, Ankbot resolves to a loopback address: 127.0.1.1; using 192.168.1.140 instead (on interface wlo1)\n",
      "24/09/04 08:32:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/taoufik/.ivy2/cache\n",
      "The jars for the packages stored in: /home/taoufik/.ivy2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6db1dbb8-abe2-42ab-8c86-7f8c435577bc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.antlr#antlr4-runtime;4.9.3 in local-m2-cache\n",
      ":: resolution report :: resolve 1164ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   1   |   1   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6db1dbb8-abe2-42ab-8c86-7f8c435577bc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "24/09/04 08:32:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/04 08:32:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../libs'))\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from init import spark\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6ea191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tenant_id': 'root',\n",
       " 'workspace_id': 'root',\n",
       " 'table_prefix': '',\n",
       " 'project_id': 'project1',\n",
       " 'import_id': 'import1',\n",
       " 'job_id': 'job1'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from context import initialize_context\n",
    "\n",
    "context = initialize_context()\n",
    "\n",
    "(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868394ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 18, 54, 162, 486]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataproject import DataProject\n",
    "data_project = DataProject(context[\"project_id\"])\n",
    "configuration = data_project.getConfiguration()\n",
    "\n",
    "content_clusters = [6, 18, 54, 162, 486]\n",
    "\n",
    "(content_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11af7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|project_id|content_group|                  id|         original_id|line|             content|               dim_1|               dim_2|               dim_3|dim_4|dim_5|dim_6|dim_7|dim_8|dim_9|dim_10|dim_11|dim_12|dim_13|dim_14|dim_15|dim_16|dim_17|dim_18|dim_19|dim_20|dim_21|dim_22|dim_23|dim_24|dim_25|\n",
      "+----------+-------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|  project1|      import1|00b4b3d5-62ff-48c...|tag:search.twitte...|   0|« si vous entende...|3c5920af22f42bf2e...|2132e265fda1a60ac...|b858a87c07b04c456...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|052a78e9-8cf5-426...|tag:search.twitte...|   0|\"....conflict. Fr...|a083175563a4b2e6e...|2646e2c407378e657...|094b0fe0e302854af...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|055bfad4-f5b8-4b9...|tag:search.twitte...|   0|Es Magia de Raspu...|44746917769308395...|5680d9c199a0ee358...|09cd68a2a77b22a31...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|06baff30-3aff-422...|tag:search.twitte...|   0|Man pat?k š? argu...|bda1b58d3a850d353...|e6afaeafef0918538...|4d73c29a4561048ce...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|01b12a92-0ca0-458...|tag:search.twitte...|   0|#UkraineRussiaWar...|ae31c6f34af382969...|738bd1fc78fbc4032...|f437cb078acc7c6d7...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|01578d8f-d67f-45a...|tag:search.twitte...|   0|Hello, I would li...|1f7f17f21b08ee03a...|acc505cb68b4a7916...|094b0fe0e302854af...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|0040eac9-4bf6-4f1...|tag:search.twitte...|   0|„Macron waha si?,...|e33081ad0a00baec9...|eb0980cf54bf2c751...|f437cb078acc7c6d7...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|05dc90bb-2bd8-403...|tag:search.twitte...|   0|Ukraine and Franc...|b27b49a926653f92c...|c3ecaa1bfb2170f2e...|094b0fe0e302854af...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|00298142-4f16-45c...|tag:search.twitte...|   0|#Ukraine un envoi...|d6d1efff78992db0b...|537c069db676f0317...|b858a87c07b04c456...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|054a48fe-29d8-44b...|tag:search.twitte...|   0|#macron ne peut a...|299b9efb3a258eaeb...|edbf90b8e87e1a803...|b858a87c07b04c456...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|03e4f52b-9052-4ea...|tag:search.twitte...|   0|Président de la R...|a31a7a3fe8fefc0ed...|137504d68f825e2a6...|b858a87c07b04c456...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|06edbd6f-f951-45e...|tag:search.twitte...|   0|@gpward72 @Helios...|22c791c8ef0c8dbeb...|697085a6564aa3b53...|094b0fe0e302854af...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|057fd033-f52c-443...|tag:search.twitte...|   0|France has handed...|c501d3ff7bad0d644...|15b8a4f03d09ef067...|094b0fe0e302854af...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|014c1b52-f916-4e9...|tag:search.twitte...|   0|For once I hear t...|2d7763c7164a1f05e...|d9510bf9699c18d15...|094b0fe0e302854af...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|030c7f20-0a9e-472...|tag:search.twitte...|   0|Halte là ! Qui va...|7ff70b5850f6f360d...|e15994a0208a9c0db...|b858a87c07b04c456...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|01319ba1-ae2f-4d7...|tag:search.twitte...|   0|\"Dania og?asza, ?...|c32de3170271556f5...|2af4162df592d7468...|f437cb078acc7c6d7...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|01e26d64-cbe3-435...|tag:search.twitte...|   0|Une erreur de plu...|78b08cd5d28ea12b2...|aa5ed93b8837ce063...|b858a87c07b04c456...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|07835173-d7f3-4f6...|tag:search.twitte...|   0|@AQuatennens vous...|8782e6d613cdf1395...|2a5844c8ee3da8caa...|b858a87c07b04c456...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|01f97d76-2385-482...|tag:search.twitte...|   0|@P_Kallioniemi  -...|eae3eb8663e05fb74...|f8fcc4d9a709792e5...|094b0fe0e302854af...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "|  project1|      import1|06444f4a-6979-451...|tag:search.twitte...|   0|  #StandWithUkraine?|e184461facc4fce4d...|788577ee32274109a...|a9dd64084afe2ab8b...| NULL| NULL| NULL| NULL| NULL| NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|  NULL|\n",
      "+----------+-------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contents = spark.sql(f\"\"\"SELECT * FROM norm_contents WHERE project_id='{context[\"project_id\"]}' AND content_group='{context[\"import_id\"]}'\"\"\")\n",
    "contents = contents.limit(1000)\n",
    "contents.show()\n",
    "display(contents.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294740ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 08:33:12.457758: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-04 08:33:12.474042: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-04 08:33:12.592649: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-04 08:33:13.130974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5231218339f04be7aab767788e802cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taoufik/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<FlagEmbedding.bge_m3.BGEM3FlagModel at 0x716234793d10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAGIC %pip install peft\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model = BGEM3FlagModel('BAAI/bge-m3') #, use_fp16=True)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483688f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 08:33:27.768497: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-04 08:33:27.769611: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-04 08:33:27.790384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-04 08:33:28.100079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/taoufik/.local/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pacmap\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "\"\"\"def compute_embeddings():\n",
    "    content_texts = [str(content['content']) for content in contents.select(\n",
    "        col('content')\n",
    "    ).collect()]\n",
    "\n",
    "    return  pd.DataFrame(\n",
    "        model.encode(content_texts, return_dense=True)[\"dense_vecs\"]\n",
    "    )\n",
    "\n",
    "\n",
    "content_embeddings =  compute_embeddings()\n",
    "\"\"\"\n",
    "\n",
    "# Assuming `model.encode` is a function that works on a list of strings and returns a list of dense vectors\n",
    "\n",
    "# Define the UDF using pandas_udf\n",
    "# @pandas_udf(\"array<float>\", PandasUDFType.SCALAR_ITER)\n",
    "#@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR)\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def encode_udf(content_series: pd.Series) -> pd.Series:\n",
    "    # Convert the series to a list of strings\n",
    "    content_list = content_series.astype(str).tolist()\n",
    "    \n",
    "    # Encode the list of strings to get a list of dense vectors\n",
    "    dense_vecs = model.encode(content_list, return_dense=True)[\"dense_vecs\"]\n",
    "    \n",
    "    # Ensure each dense_vec is flattened and converted to a list\n",
    "    flat_dense_vecs = [vec.tolist() for vec in dense_vecs]\n",
    "    \n",
    "    # Return the results as a pandas Series\n",
    "    return pd.Series(flat_dense_vecs)\n",
    "    \n",
    "# Assuming you have a Spark session and contents DataFrame\n",
    "# Apply the UDF to the 'content' column to get the embeddings\n",
    "content_embeddings_df = contents.withColumn(\"embedding\", encode_udf(col(\"content\")))\n",
    "\n",
    "# Collect the embeddings into a local DataFrame\n",
    "content_embeddings = content_embeddings_df.select(\"embedding\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cffd88e-6ec2-4134-a64a-080ee176ed73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   1,   0, ...,   1,   1,   1],\n",
       "       ...,\n",
       "       [  0,   1,   2, ..., 483, 466, 466],\n",
       "       [  0,   1,   2, ..., 484, 467, 467],\n",
       "       [  0,   1,   2, ..., 485, 468, 468]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607c7b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taoufik/.local/lib/python3.11/site-packages/genieclust/genie.py:193: UserWarning: `compute_full_tree` is only available when `M` = 1 and `exact` is True\n",
      "  warnings.warn(\"`compute_full_tree` is only available when `M` = 1 \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k6</th>\n",
       "      <th>k18</th>\n",
       "      <th>k54</th>\n",
       "      <th>k162</th>\n",
       "      <th>k486</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>146</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>159</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>146</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>161</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     k6  k18  k54  k162  k486\n",
       "0     0    0    0     0     0\n",
       "1     1    1    1     1     1\n",
       "2     2    2    2     2     2\n",
       "3     3    3    3     3     3\n",
       "4     2    2    2     2     4\n",
       "..   ..  ...  ...   ...   ...\n",
       "986   5   17   53   146   481\n",
       "987   5   16   50   159   482\n",
       "989   5   17   53   146   483\n",
       "991   5   16   50   160   484\n",
       "997   5   16   50   161   485\n",
       "\n",
       "[486 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import functools\n",
    "import genieclust\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, lit, to_number\n",
    "\n",
    "def compute_content_clustering(input, clusters):\n",
    "    return genieclust.Genie(\n",
    "            n_clusters=clusters, cast_float32=True, gini_threshold=0.2,\n",
    "            affinity=\"cosinesimil\", exact=False, compute_all_cuts=True, compute_full_tree=True\n",
    "        ).fit_predict(\n",
    "           input\n",
    "        )\n",
    "\n",
    "def compute_content_clustering_hierarchy(clustered_content):\n",
    "    return pd.DataFrame(\n",
    "        np.transpose( clustered_content[content_clusters, :]),\n",
    "        columns=[f'k{content_cluster}' for content_cluster in content_clusters]\n",
    "    ).drop_duplicates()\n",
    "\n",
    "content_embeddings_array = np.array([row.embedding for row in content_embeddings], dtype=np.float32)\n",
    "\n",
    "clustered_content = compute_content_clustering(content_embeddings_array, content_clusters[-1])\n",
    "cluster_hierarchy = compute_content_clustering_hierarchy(clustered_content)\n",
    "\n",
    "\n",
    "display(cluster_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d28e1b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[project_id: string, content_group: string, content_id: string, content_topic: int, content_topic_ancestor: int, content_topic_clusters: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[project_id: string, content_group: string, content_id: string, content_topic: int, content_topic_ancestor: int, content_topic_clusters: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[project_id: string, content_group: string, content_id: string, content_topic: int, content_topic_ancestor: int, content_topic_clusters: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[project_id: string, content_group: string, content_id: string, content_topic: int, content_topic_ancestor: int, content_topic_clusters: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[project_id: string, content_group: string, content_id: string, content_topic: int, content_topic_ancestor: int, content_topic_clusters: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "\n",
    "def compute_content_topics(contents, content_clusters, index, clustered_content):\n",
    "    content_ids = contents.select(col(\"id\")).toPandas()\n",
    "    content_ids[\"content_topic\"] = clustered_content[content_clusters[index], :]\n",
    "    if index == 0:\n",
    "        content_ids[\"content_topic_ancestor\"] = np.repeat(None, len(content_ids.index))\n",
    "    else:\n",
    "        content_ids[\"content_topic_ancestor\"] = clustered_content[content_clusters[index - 1], :]\n",
    "    return content_ids\n",
    "\n",
    "content_topics = [[content_cluster, compute_content_topics(contents, content_clusters, index, clustered_content)] for [index, content_cluster] in enumerate(content_clusters)]\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_content_clusterings\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "          content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"content_topic\", IntegerType(), True),\n",
    "    StructField(\"content_topic_ancestor\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "for [content_cluster, content_topic] in content_topics:\n",
    "    final = spark.createDataFrame(content_topic, schema=schema).select(\n",
    "        lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "        lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "        col(\"id\").alias(\"content_id\"),\n",
    "        col(\"content_topic\").cast(\"INTEGER\"),\n",
    "        col(\"content_topic_ancestor\").cast(\"INTEGER\"),\n",
    "        lit(content_cluster).alias(\"content_topic_clusters\").cast(\"INTEGER\")\n",
    "    )\n",
    "    \n",
    "    final.writeTo(f\"\"\"comp_content_clusterings\"\"\").using(\"delta\").append()\n",
    "\n",
    "    display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b773d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workspace.progress(context['job_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28941ad5-ccb6-4deb-a452-74ea15ddb04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----+\n",
      "|    data_1|content_topic|count|\n",
      "+----------+-------------+-----+\n",
      "|2023-07-12|            1|    3|\n",
      "|2023-10-31|            0|    1|\n",
      "|2023-03-29|            9|    1|\n",
      "|2023-05-28|            5|    1|\n",
      "|2023-08-21|            2|    1|\n",
      "|2023-03-02|            6|    3|\n",
      "|2023-08-15|            3|    1|\n",
      "|2023-01-31|            4|   24|\n",
      "|2023-07-12|            5|   21|\n",
      "|2023-01-21|            0|    4|\n",
      "|2023-03-24|            4|    1|\n",
      "|2023-09-23|            6|    1|\n",
      "|2023-08-26|            3|    1|\n",
      "|2023-03-31|            6|    1|\n",
      "|2023-02-18|            4|    4|\n",
      "|2023-01-31|            5|    4|\n",
      "|2023-09-06|            5|    1|\n",
      "|2023-09-05|            8|    1|\n",
      "|2022-10-02|            7|    1|\n",
      "|2023-06-13|            0|    1|\n",
      "+----------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+\n",
      "|content_topic|\n",
      "+-------------+\n",
      "|            1|\n",
      "|            6|\n",
      "|            3|\n",
      "|            5|\n",
      "|            9|\n",
      "|            4|\n",
      "|            8|\n",
      "|            7|\n",
      "|            2|\n",
      "|            0|\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 719:==============================================>      (138 + 1) / 157]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|content_topic_clusters|\n",
      "+----------------------+\n",
      "|                    54|\n",
      "|                   486|\n",
      "|                    18|\n",
      "|                   162|\n",
      "|                     6|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/04 15:21:13 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-c6285f8a-7870-43c7-8dc9-89c9bf2908d5. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-c6285f8a-7870-43c7-8dc9-89c9bf2908d5\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 45586)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/taoufik/.local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/taoufik/.local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/taoufik/.local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/taoufik/.local/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#df = spark.sql(\"select * from norm_content_dims where ...\")\n",
    "df = spark.sql(\"\"\"SELECT\n",
    "  norm_content_dims.data_1,\n",
    "  comp_content_clusterings.content_topic,\n",
    "  COUNT(norm_contents.id) AS count\n",
    "FROM norm_content_dims\n",
    "JOIN norm_contents ON norm_content_dims.id = norm_contents.dim_2\n",
    "JOIN comp_content_clusterings ON norm_contents.id = comp_content_clusterings.content_id\n",
    "WHERE norm_contents.project_id = 'project1' AND\n",
    "      norm_content_dims.project_id = 'project1' AND\n",
    "      comp_content_clusterings.project_id = 'project1' AND\n",
    "      dimension = 2 AND\n",
    "      comp_content_clusterings.content_topic_clusters = 18\n",
    "GROUP BY content_topic, data_1\"\"\")\n",
    "df.show()\n",
    "df.select(\"content_topic\").distinct().show()\n",
    "spark.sql(\"select content_topic_clusters from comp_content_clusterings\").distinct().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
