{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02235355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../libs'))\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from init import spark\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import initialize_context\n",
    "\n",
    "context = initialize_context()\n",
    "\n",
    "(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed516378",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_clusters = [20, 60, 180, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        norm_contents.project_id, norm_contents.content_group,\n",
    "        norm_contents.content,\n",
    "        comp_content_clusterings.content_topic_clusters, comp_content_clusterings.content_topic\n",
    "    FROM norm_contents\n",
    "    JOIN comp_content_clusterings ON\n",
    "         comp_content_clusterings.project_id = norm_contents.project_id AND\n",
    "         comp_content_clusterings.content_group = norm_contents.content_group AND\n",
    "         comp_content_clusterings.content_id = norm_contents.id \n",
    "    WHERE norm_contents.project_id='{context[\"project_id\"]}' AND\n",
    "          norm_contents.content_group='{context[\"import_id\"]}'\n",
    "\"\"\").limit(100)\n",
    "\n",
    "display(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44537e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "contents_per_topics = contents.groupBy(\n",
    "    ['project_id', 'content_group', 'content_topic_clusters', 'content_topic']\n",
    ").agg(\n",
    "    collect_list('content').alias(\"contents\"),\n",
    ")\n",
    "\n",
    "display('contents_per_topic')\n",
    "display(contents_per_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dff526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "num_words=250\n",
    "@udf(returnType=MapType(StringType(), IntegerType()))\n",
    "def top_words_udf(contents):\n",
    "    all_stop_words = set()\n",
    "    #for lang in stopwords.fileids():  \n",
    "    #    all_stop_words.update(stopwords.words(lang))\n",
    "    stop_words_list = list(all_stop_words)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words_list,lowercase=True, max_features=100, strip_accents='unicode')\n",
    "    word_counts = vectorizer.fit_transform(contents).toarray().sum(axis=0)\n",
    "    top_word_indices = word_counts.argsort()[::-1][:num_words]\n",
    "    top_words = { vectorizer.get_feature_names_out()[i]: int(word_counts[i]) for i in top_word_indices }\n",
    "\n",
    "    return top_words\n",
    "\n",
    "terms_per_topics = contents_per_topics.withColumn(\n",
    "    \"terms\", top_words_udf(\"contents\")\n",
    ").drop(\"contents\").selectExpr(\n",
    "    \"*\", \"explode(terms) as (term,count)\"\n",
    ").drop(\"terms\")\n",
    "\n",
    "display('terms_per_topics')\n",
    "display(terms_per_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196731ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "terms_count_per_topic = terms_per_topics.groupBy(\n",
    "    \"project_id\", \"content_group\", \"content_topic_clusters\", \"content_topic\"\n",
    ").agg(sum(\"count\").alias(\"count_sum\"))\n",
    "\n",
    "display('terms_count_per_topic')\n",
    "display(terms_count_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_per_topics_with_score = terms_per_topics.join(\n",
    "    terms_count_per_topic, on=[\"project_id\", \"content_group\", \"content_topic_clusters\", \"content_topic\"]\n",
    ").selectExpr(\n",
    "    \"*\", \"count/count_sum as score\"\n",
    ").drop(\"count\", \"count_sum\")\n",
    "\n",
    "display('terms_per_topics_with_score')\n",
    "terms_per_topics_with_score.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_content_terms\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "          content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "terms_per_topics_with_score.writeTo(f\"\"\"comp_content_terms\"\"\").using(\"delta\").append()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
