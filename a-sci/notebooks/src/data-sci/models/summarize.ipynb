{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50450c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "class SummarizeModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        from gritlm import GritLM\n",
    "        self.model = GritLM(\"GritLM/GritLM-7B\", torch_dtype=\"auto\")\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        def predict_prompt(prompt, content, max):\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}: {content}\"}\n",
    "            ]\n",
    "            encoded = self.model.tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(self.model.device)\n",
    "            gen = self.model.generate(encoded, max_new_tokens=max, do_sample=False)\n",
    "            decoded = self.model.tokenizer.batch_decode(gen)\n",
    "            return decoded[0].split('<|assistant|>\\n')[1].split('</s>')[0]\n",
    "\n",
    "        large = predict_prompt(model_input['large_prompt'], model_input['text'], 5000)\n",
    "        medium = predict_prompt(model_input['medium_prompt'], large, 1000)\n",
    "        small = predict_prompt(model_input['small_prompt'], medium, 100)\n",
    "\n",
    "        return { 'large': large, 'medium': medium, 'small': small }"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
