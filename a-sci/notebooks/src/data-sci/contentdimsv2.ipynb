{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f086a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../libs'))\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from init import spark\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a095be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import initialize_context\n",
    "\n",
    "context = initialize_context()\n",
    "\n",
    "(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8107f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataproject import DataProject\n",
    "data_project = DataProject(context[\"project_id\"])\n",
    "configuration = data_project.getConfiguration()\n",
    "\n",
    "\n",
    "configured_dimensions =  configuration['dimensions']['value']\n",
    "content_clusters = [20, 60, 180, 500]\n",
    "content_clusters_components = configuration['content_cluster_components']['value']\n",
    "content_dims_components = configuration['content_dim_components']['value']\n",
    "\n",
    "print(configured_dimensions, content_clusters, content_clusters_components, content_dims_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        norm_contents.*,\n",
    "        comp_content_clusterings.content_topic_clusters, comp_content_clusterings.content_topic_ancestor,\n",
    "        comp_content_clusterings.content_topic\n",
    "    FROM norm_contents\n",
    "    JOIN comp_content_clusterings\n",
    "        ON comp_content_clusterings.project_id = norm_contents.project_id AND\n",
    "           comp_content_clusterings.content_group = norm_contents.content_group AND\n",
    "           comp_content_clusterings.content_id = norm_contents.id\n",
    "    WHERE norm_contents.project_id='{context[\"project_id\"]}' AND\n",
    "          norm_contents.content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "contents.limit(5).show(truncate=False)\n",
    "display(contents.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90976b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "def compute_dim(contents, dim, cluster):\n",
    "    dim_col = f\"dim_{dim}\"\n",
    "    dim_col_alias = f\"i_{dim_col}\"\n",
    "    content_topic_col_alias = \"i_content_topic\"\n",
    "\n",
    "    contents_per_dim = contents.where(f'content_topic_clusters = {cluster}').groupBy(dim_col).count().select(\n",
    "        col(dim_col),\n",
    "        col('count').alias('total_size')\n",
    "    ).where(f\"{dim_col} IS NOT NULL\")\n",
    "\n",
    "    if contents_per_dim.count() == 0:\n",
    "        return None\n",
    "\n",
    "    print('contents_per_dim')\n",
    "    contents_per_dim.show(5, truncate=False)\n",
    "\n",
    "    contents_per_topics_per_dim = contents.groupBy([dim_col, \"content_topic\"]).count().select(\n",
    "        col(dim_col).alias(dim_col_alias),\n",
    "        col(\"content_topic\"),\n",
    "        col('count').alias('topic_size')\n",
    "    )\n",
    "    joined_contents_per_topics_per_dim = contents_per_dim.join(\n",
    "        contents_per_topics_per_dim, contents_per_dim[dim_col] == contents_per_topics_per_dim[dim_col_alias], \"inner\"\n",
    "    ).select(\n",
    "        col(dim_col),\n",
    "        col(\"content_topic\"),\n",
    "        col('total_size'),\n",
    "        col('topic_size')\n",
    "    )\n",
    "\n",
    "    print('contents_per_topics_per_dim')\n",
    "    contents_per_topics_per_dim.show(5, truncate=False)\n",
    "\n",
    "    print('joined_contents_per_topics_per_dim')\n",
    "    joined_contents_per_topics_per_dim.show(5,truncate=False)\n",
    "\n",
    "    contents.select(\n",
    "        col(\"content_topic\"),\n",
    "        col(dim_col)\n",
    "    ).show(5,truncate=False)\n",
    "\n",
    "    panda_contents = contents.select(\n",
    "        col(\"content_topic\"),\n",
    "        col(dim_col)\n",
    "    ).toPandas()\n",
    "    panda_contents_per_dim = panda_contents.groupby(dim_col).size()\n",
    "    print('panda_contents_per_dim')\n",
    "    print(panda_contents_per_dim)\n",
    "    panda_contents_per_topics_per_dims = panda_contents.pivot_table(\n",
    "        index=dim_col, columns='content_topic', aggfunc='size', fill_value=0\n",
    "    )\n",
    "    print('panda_contents_per_topics_per_dims')\n",
    "    print(panda_contents_per_topics_per_dims)\n",
    "\n",
    "    panda_content_proportion_per_topics_per_dims = panda_contents_per_topics_per_dims.divide(panda_contents_per_dim, axis=0)\n",
    "    print('panda_content_proportion_per_topics_per_dims')\n",
    "    print(panda_content_proportion_per_topics_per_dims)\n",
    "\n",
    "    panda_impact_per_topics_per_dims = panda_contents_per_topics_per_dims * panda_content_proportion_per_topics_per_dims\n",
    "\n",
    "    print('panda_impact_per_topics_per_dims', cluster)\n",
    "    display(panda_impact_per_topics_per_dims)\n",
    "\n",
    "    num_topics = panda_contents_per_topics_per_dims.shape[1]\n",
    "    value_vars = [i for i in range(min(num_topics,cluster))]\n",
    "    panda_melted_impact_per_topics_per_dims =  spark.createDataFrame(\n",
    "        pd.melt(\n",
    "            panda_impact_per_topics_per_dims.reset_index(), id_vars=dim_col, value_vars=value_vars, var_name='content_topic', value_name='value'\n",
    "        )\n",
    "    ).select(\n",
    "        col(dim_col).alias(dim_col_alias),\n",
    "        col(\"content_topic\").alias(content_topic_col_alias),\n",
    "        col('value').alias('score')\n",
    "    )\n",
    "    scored_contents_per_topics_per_dim = joined_contents_per_topics_per_dim.join(panda_melted_impact_per_topics_per_dims, [\n",
    "        joined_contents_per_topics_per_dim[dim_col] == panda_melted_impact_per_topics_per_dims[dim_col_alias],\n",
    "        joined_contents_per_topics_per_dim['content_topic'] == panda_melted_impact_per_topics_per_dims[content_topic_col_alias]\n",
    "    ], \"inner\").select(\n",
    "        lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "        lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "        lit(cluster).alias(\"content_topic_clusters\"),\n",
    "        col(\"content_topic\"),\n",
    "        lit(dim).alias(\"dimension\"),\n",
    "        col(dim_col).alias(\"dimension_id\"),\n",
    "        col('total_size').cast(\"INTEGER\"),\n",
    "        col('topic_size').cast(\"INTEGER\"),\n",
    "        col('score').cast(\"DOUBLE\")\n",
    "    )\n",
    "\n",
    "    return (scored_contents_per_topics_per_dim, panda_content_proportion_per_topics_per_dims)\n",
    "\n",
    "content_topic_dims_clusters = [\n",
    "    [compute_dim(contents, index + 1, cluster) for index, dim in enumerate(configured_dimensions)]\n",
    "    for cluster in content_clusters\n",
    "]\n",
    "\n",
    "display('content_topic_dims_clusters', content_topic_dims_clusters)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_content_topic_dims\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "        content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "for content_topic_dims in content_topic_dims_clusters:\n",
    "    for index, dim in enumerate(content_topic_dims):\n",
    "        if dim is not None:\n",
    "            dim[0].writeTo(f\"\"\"comp_content_topic_dims\"\"\").using(\"delta\").append()\n",
    "            display(dim[0].limit(50))\n",
    "            display(dim[0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b53e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def compute_dim_pca(panda_content_proportion_per_topics_per_dims, dim, cluster):\n",
    "    if len(panda_content_proportion_per_topics_per_dims.index) < content_dims_components:\n",
    "        return (spark.createDataFrame([], StructType([])), 0, 0)\n",
    "\n",
    "    dim_col = f\"dim_{dim}\"\n",
    "    standardized_content_proportion_per_topics_per_dims = scaler.fit_transform(panda_content_proportion_per_topics_per_dims)\n",
    "    dim_pca = PCA(n_components=content_dims_components)\n",
    "    dim_principal_components = dim_pca.fit_transform(standardized_content_proportion_per_topics_per_dims)\n",
    "    dim_principal_components_df = pd.DataFrame(\n",
    "        data=dim_principal_components,\n",
    "        columns=[f'PC{index +1}' for index in range(content_dims_components)],\n",
    "        index=panda_content_proportion_per_topics_per_dims.index\n",
    "    )\n",
    "    dim_principal_components_df[dim_col] = dim_principal_components_df.index\n",
    "\n",
    "    return (\n",
    "        spark.createDataFrame(dim_principal_components_df).select(\n",
    "            lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "            lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "            lit(cluster).alias(\"content_topic_clusters\"),\n",
    "            lit(dim).alias(\"dimension\"),\n",
    "            col(dim_col).alias(\"dimension_id\"),\n",
    "            *[col(f'PC{index +1}').alias(f'pc{index + 1}') for index in range(content_dims_components)]\n",
    "        ),\n",
    "        dim_pca,\n",
    "        panda_content_proportion_per_topics_per_dims\n",
    "    )\n",
    "\n",
    "dims_clusters = [\n",
    "    [\n",
    "        compute_dim_pca(content_topic_dims[index][1], index + 1, content_clusters[cluster_index])\n",
    "        for index, dim in enumerate(configured_dimensions)\n",
    "        if content_topic_dims[index] is not None\n",
    "    ]\n",
    "    for cluster_index, content_topic_dims\n",
    "    in enumerate(content_topic_dims_clusters)\n",
    "]\n",
    "\n",
    "display('dims_clusters', dims_clusters)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_content_dim_clusterings\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "        content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "for dims in dims_clusters:\n",
    "    for index, dim in enumerate(dims):\n",
    "        dim[0].writeTo(f\"\"\"comp_content_dim_clusterings\"\"\").using(\"delta\").append()\n",
    "\n",
    "        display(dim[0].limit(50))\n",
    "        display(dim[0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ebe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_content_dim_topics(dim_pca, panda_content_proportion_per_topics_per_dims, dim, cluster):\n",
    "    if dim_pca == 0:\n",
    "        return spark.createDataFrame([], StructType([]))\n",
    "\n",
    "    topics_loadings = dim_pca.components_.T  # Transpose to align with original variables\n",
    "    topics_loadings_df = pd.DataFrame(data=topics_loadings, columns=['PC1', 'PC2'], index=panda_content_proportion_per_topics_per_dims.columns)\n",
    "    topics_loadings_df['content_topic'] = topics_loadings_df.index\n",
    "    topics_loadings_spark_df = spark.createDataFrame(topics_loadings_df)\n",
    "\n",
    "    dim_topics_stats = spark.sql(f\"\"\"\n",
    "        SELECT content_topic as i_content_topic, sum(total_size) as total_size, sum(topic_size) as topic_size, min(score) as min_score, max(score) as max_score\n",
    "        FROM comp_content_topic_dims\n",
    "        WHERE comp_content_topic_dims.project_id='{context[\"project_id\"]}' AND\n",
    "              comp_content_topic_dims.content_group='{context[\"import_id\"]}' AND\n",
    "              comp_content_topic_dims.content_topic_clusters={cluster} AND\n",
    "              comp_content_topic_dims.dimension={dim}\n",
    "        GROUP BY content_topic\n",
    "    \"\"\")\n",
    "\n",
    "    return topics_loadings_spark_df.join(\n",
    "        dim_topics_stats,dim_topics_stats.i_content_topic == topics_loadings_spark_df.content_topic, \"inner\"\n",
    "    ).select(\n",
    "        lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "        lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "        lit(cluster).alias(\"content_topic_clusters\"),\n",
    "        col(\"content_topic\").cast(\"INTEGER\"),\n",
    "        lit(dim).alias(\"dimension\"),\n",
    "        *[col(f'PC{index +1}').alias(f'pc{index + 1}') for index in range(content_dims_components)],\n",
    "        col(\"total_size\").cast(\"INTEGER\"),\n",
    "        col(\"topic_size\").cast(\"INTEGER\"),\n",
    "        col(\"min_score\").cast(\"DOUBLE\"),\n",
    "        col(\"max_score\").cast(\"DOUBLE\"),\n",
    "    )\n",
    "\n",
    "content_dim_topics_clusters = [\n",
    "    [\n",
    "        compute_content_dim_topics(dims[index][1], dims[index][2], index + 1, content_clusters[cluster_index])\n",
    "        for index, dim in enumerate(configured_dimensions)\n",
    "        if index < len(dims)\n",
    "    ]\n",
    "    for cluster_index, dims in enumerate(dims_clusters)\n",
    "]\n",
    "\n",
    "display('content_dim_topics_clusters', content_dim_topics_clusters)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_content_dim_content_topics\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "        content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "for content_dim_topics in content_dim_topics_clusters:\n",
    "    for index, dim in enumerate(content_dim_topics):\n",
    "        dim.writeTo(f\"\"\"comp_content_dim_content_topics\"\"\").using(\"delta\").append()\n",
    "\n",
    "        display(dim.limit(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
