{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../libs'))\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from init import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import initialize_context\n",
    "\n",
    "context = initialize_context()\n",
    "\n",
    "(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataproject import DataProject\n",
    "data_project = DataProject(context[\"project_id\"])\n",
    "configuration = data_project.getConfiguration()\n",
    "\n",
    "data_project_import = {'importId': 'nGPTuosT6Z_jLb3gB1-Yk', \n",
    "  'params': {'format': {'quote': '\"',\n",
    "    'escape': '\"',\n",
    "    'encoding': 'UTF-8',\n",
    "    'delimiter': ';',\n",
    "    'quoteEscape': '\"'},\n",
    "   'mapping': [{'type': 'text', 'input': 'text'},\n",
    "    {'type': 'text', 'input': 'original id'},\n",
    "    {'type': 'text', 'input': 'user'},\n",
    "    {'type': 'date', 'input': 'date', 'format': 'dd/MM/yyyy HH:mm'},\n",
    "    {'type': 'text', 'input': 'lang'}]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number,lit,col\n",
    "from pyspark.sql.window import Window\n",
    "w = Window().orderBy(lit('A'))\n",
    "\n",
    "raw_contents = spark.read.format('csv').options(\n",
    "    header='True',\n",
    "    delimiter=data_project_import['params']['format']['delimiter'],\n",
    "    escape=data_project_import['params']['format']['escape'],\n",
    "    quote=data_project_import['params']['format']['quote'],\n",
    "    multiline='True'\n",
    ").load(\n",
    "    f'/home/taoufik/workspace/arlequin/tenant_{context[\"tenant_id\"]}/workspace_{context[\"workspace_id\"]}/raw/{data_project_import[\"importId\"]}'\n",
    ").withColumn(\"row_num\", row_number().over(w))\n",
    "\n",
    "display(raw_contents.limit(100))\n",
    "display(raw_contents.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6522620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, lit, to_timestamp, udf, posexplode, sha1, date_format\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "def chunk_string(input):\n",
    "    n = 3072\n",
    "    if type(input) is str:\n",
    "        return [input[i:i+n] for i in range(0, len(input), n)]\n",
    "    return input\n",
    "\n",
    "chunk_string_udf = udf(lambda str: chunk_string(str),  ArrayType(StringType()))\n",
    "\n",
    "def content_col():\n",
    "    return posexplode(\n",
    "        chunk_string_udf(data_project_import['params']['mapping'][0]['input'])\n",
    "    ).alias('line', 'content')\n",
    "\n",
    "def original_id_col():\n",
    "    if len(data_project_import['params']['mapping'][1]['input']) > 0:\n",
    "        return col(\n",
    "            data_project_import['params']['mapping'][1]['input']\n",
    "        ).alias(\"original_id\")\n",
    "    else:\n",
    "        return col('row_num').cast(\"STRING\").alias(\"original_id\")\n",
    "\n",
    "def dim_col(mapping, index):\n",
    "    if mapping['type'] == 'date':\n",
    "        return date_format(\n",
    "            to_timestamp(col(mapping['input']), mapping['format']),\n",
    "            \"yyyy-MM-dd\"\n",
    "        ).alias(f\"r_dim_{index}\")\n",
    "    else:\n",
    "        return col(mapping['input']).alias(f\"r_dim_{index}\")\n",
    "    \n",
    "def dim_cols():\n",
    "    return [dim_col(mapping, index - 1) for index, mapping in enumerate(data_project_import['params']['mapping']) if index > 1]\n",
    "\n",
    "cleaned_contents = raw_contents.withColumn(\n",
    "    \"project_id\", lit(context[\"project_id\"])\n",
    ").withColumn(\n",
    "    \"content_group\", lit(context[\"import_id\"])\n",
    ").withColumn(\n",
    "    \"id\",  expr(\"uuid()\")\n",
    ").select(\n",
    "    col(\"project_id\"),\n",
    "    col(\"content_group\"),\n",
    "    col(\"id\"),\n",
    "    original_id_col(),\n",
    "    content_col(),\n",
    "    *dim_cols()\n",
    ").where(\n",
    "    \"content IS NOT NULL\"\n",
    ")\n",
    "\n",
    "display(cleaned_contents.limit(100))\n",
    "display(cleaned_contents.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(configuration)\n",
    "configured_dimensions = configuration['dimensions']['value']\n",
    "\n",
    "def aggregate_dim(dim, index):\n",
    "    r_dim = f\"r_dim_{index + 1}\"\n",
    "\n",
    "    return cleaned_contents.groupBy(r_dim).agg({r_dim: 'count'}).select(\n",
    "        lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "        lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "        lit(index + 1).alias(\"dimension\"),\n",
    "        sha1(r_dim).alias(\"id\"),\n",
    "        col(r_dim).cast(\"STRING\").alias(\"data_1\"),\n",
    "        col(f\"count({r_dim})\").cast('INTEGER').alias(\"count\")\n",
    "    ).where(\"id IS NOT NULL\")\n",
    "\n",
    "dimensions = [aggregate_dim(dim, index) for index, dim in enumerate(configured_dimensions)]\n",
    "\n",
    "for index, dimension in enumerate(dimensions):\n",
    "    spark.sql(f\"\"\"\n",
    "        DELETE FROM norm_content_dims\n",
    "        WHERE project_id='{context[\"project_id\"]}' AND\n",
    "              content_group='{context[\"import_id\"]}' AND\n",
    "              dimension={index + 1}\n",
    "    \"\"\")\n",
    "    dimension.writeTo(f\"\"\"norm_content_dims\"\"\").using(\"delta\").append()\n",
    "    display(dimension.limit(50))\n",
    "    display(dimension.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def join_dim(joined_contents, dim, index):\n",
    "    if dim.count() == 0:\n",
    "        return joined_contents\n",
    "    \n",
    "    j_dim = f\"j_dim_{index + 1}\"\n",
    "    prepared_dim = dim.select(col(\"id\").alias(f\"dim_{index + 1}\"), col(\"data_1\").alias(j_dim))\n",
    "    return joined_contents.join(prepared_dim, prepared_dim[1].eqNullSafe(joined_contents[6 + index]), \"outer\")\n",
    "\n",
    "joined_contents = functools.reduce(lambda joined_contents, dim: join_dim(joined_contents, dim[1], dim[0]), enumerate(dimensions), cleaned_contents)\n",
    "\n",
    "normalized_contents = joined_contents.select(\n",
    "    col(\"project_id\"),\n",
    "    col(\"content_group\"),\n",
    "    col(\"id\"),\n",
    "    col(\"original_id\"),\n",
    "    col(\"line\"),\n",
    "    col(\"content\"),\n",
    "    *[col(f\"dim_{index + 1}\") for index, dim in enumerate(dimensions) if dim.count() > 0]\n",
    ")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM norm_contents\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "          content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "normalized_contents.writeTo(f\"\"\"norm_contents\"\"\").using(\"delta\").append()\n",
    "\n",
    "display(normalized_contents.limit(100))\n",
    "display(normalized_contents.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b36439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workspace.progress(context['job_id'])\n",
    "\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
