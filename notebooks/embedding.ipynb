{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "654fd72b-8f31-4536-9c5c-c0372a1067db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit, explode, posexplode, row_number, expr, collect_list\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "spark = SparkSession.builder.master('local[2]').config('spark.executor.memory','8g').config('spark.driver.memory','8g').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e863db0b-a857-4b0b-85ea-f0f62f871dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text      9999\n",
      "source    9999\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3923477/3519056133.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dx2['text'] = dx2['text'].apply(split_string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('laicite_gd.csv', delimiter=\";\")\n",
    "df['source'] = df['screen name']\n",
    "dx = df[['text','source']]\n",
    "dx2 = dx.dropna()\n",
    "print(dx2.count())\n",
    "\n",
    "def split_string(string, max_length=4096):\n",
    "    return [string[i:i + max_length] for i in range(0, len(string), max_length)]\n",
    "\n",
    "dx2['text'] = dx2['text'].apply(split_string)\n",
    "dx2 = dx2.explode('text').reset_index(drop=True)\n",
    "\n",
    "sent = dx2['text'].tolist()\n",
    "s = [str(x) for x in sent]\n",
    "len(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea516a97-e927-4ebd-959c-ee8ed635a1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f351045c73634db6848a829e3eeb7816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05966125,  0.01844968, -0.03975787, ...,  0.0197802 ,\n",
       "        -0.04212459,  0.03838256],\n",
       "       [-0.03519594,  0.0537153 , -0.02790581, ..., -0.0071175 ,\n",
       "        -0.0135484 ,  0.02308535],\n",
       "       [-0.03519594,  0.0537153 , -0.02790581, ..., -0.0071175 ,\n",
       "        -0.0135484 ,  0.02308535],\n",
       "       ...,\n",
       "       [-0.00806239,  0.03040993, -0.048115  , ..., -0.0016513 ,\n",
       "        -0.01623113,  0.00687234],\n",
       "       [-0.02088596,  0.01465792, -0.0291551 , ...,  0.03885469,\n",
       "        -0.05025212, -0.00095249],\n",
       "       [-0.03519592,  0.05371533, -0.02790579, ..., -0.0071175 ,\n",
       "        -0.01354843,  0.02308529]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\n",
    "embeddings = model.encode(s, return_dense=True,max_length=4096)[\"dense_vecs\"] # upto 8192 tokens\n",
    "embeddings     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e14e641e-ee96-4756-8d8a-67a28e0935ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbf346684f74f7fbb9bd4250a3fe721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83695066]]\n",
      "[[0.61087024]]\n"
     ]
    }
   ],
   "source": [
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\n",
    "sentence_1 = [\"Before discussing this approach, let us first understand in brief what yield is.\"]\n",
    "sentence_2 = [\"The former is used inside a function. Before discussing this approach, looking at differences, let us first understand in brief what yield is.\"]\n",
    "sentence_3 = [\"The former is used inside a function. Before discussing this approach, let us first recognize the differences.\"]\n",
    "\n",
    "embeddings_1 = model.encode(sentence_1)\n",
    "embeddings_2 = model.encode(sentence_2)\n",
    "embeddings_3 = model.encode(sentence_3)\n",
    "similarity = embeddings_1[\"dense_vecs\"] @ embeddings_2[\"dense_vecs\"].T\n",
    "print(similarity) # 0.884371\n",
    "similarity = embeddings_1[\"dense_vecs\"] @ embeddings_3[\"dense_vecs\"].T\n",
    "print(similarity) # 0.9100454\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7711368d-b324-492b-974b-10cb51d64324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.059661</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>-0.039758</td>\n",
       "      <td>-0.042060</td>\n",
       "      <td>-0.044216</td>\n",
       "      <td>-0.023241</td>\n",
       "      <td>-0.027259</td>\n",
       "      <td>0.020452</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>-0.040778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046408</td>\n",
       "      <td>-0.009997</td>\n",
       "      <td>0.039415</td>\n",
       "      <td>0.026511</td>\n",
       "      <td>0.015673</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>0.071344</td>\n",
       "      <td>0.019780</td>\n",
       "      <td>-0.042125</td>\n",
       "      <td>0.038383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.035196</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>-0.027906</td>\n",
       "      <td>-0.034745</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>-0.003791</td>\n",
       "      <td>-0.023077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.007953</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.017535</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.013548</td>\n",
       "      <td>0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035196</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>-0.027906</td>\n",
       "      <td>-0.034745</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>-0.003791</td>\n",
       "      <td>-0.023077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.007953</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.017535</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.013548</td>\n",
       "      <td>0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.017894</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>-0.025652</td>\n",
       "      <td>-0.017793</td>\n",
       "      <td>-0.038787</td>\n",
       "      <td>-0.001943</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>-0.006457</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.027378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>-0.019472</td>\n",
       "      <td>0.076911</td>\n",
       "      <td>-0.016450</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>-0.015103</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>-0.005126</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.017724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.035196</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>-0.027906</td>\n",
       "      <td>-0.034745</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>-0.003791</td>\n",
       "      <td>-0.023077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.007953</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.017535</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.013548</td>\n",
       "      <td>0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.035196</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>-0.027906</td>\n",
       "      <td>-0.034745</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>-0.003791</td>\n",
       "      <td>-0.023077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.007953</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.017535</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.013548</td>\n",
       "      <td>0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>-0.054455</td>\n",
       "      <td>-0.019663</td>\n",
       "      <td>-0.018070</td>\n",
       "      <td>-0.056120</td>\n",
       "      <td>0.045075</td>\n",
       "      <td>-0.065330</td>\n",
       "      <td>-0.018213</td>\n",
       "      <td>-0.047228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009930</td>\n",
       "      <td>-0.037306</td>\n",
       "      <td>0.032301</td>\n",
       "      <td>0.038416</td>\n",
       "      <td>-0.035205</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.050371</td>\n",
       "      <td>-0.011738</td>\n",
       "      <td>-0.034052</td>\n",
       "      <td>0.026258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-0.035196</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>-0.027906</td>\n",
       "      <td>-0.034745</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>-0.003791</td>\n",
       "      <td>-0.023077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.007952</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.017535</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.013548</td>\n",
       "      <td>0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.022459</td>\n",
       "      <td>-0.043841</td>\n",
       "      <td>0.016776</td>\n",
       "      <td>-0.048276</td>\n",
       "      <td>-0.044551</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.018274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002792</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.072303</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.036395</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>0.027492</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>-0.050224</td>\n",
       "      <td>-0.019702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-0.035196</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>-0.027906</td>\n",
       "      <td>-0.034745</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.030240</td>\n",
       "      <td>0.019474</td>\n",
       "      <td>-0.003790</td>\n",
       "      <td>-0.023077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.007953</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>-0.017535</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.002747</td>\n",
       "      <td>-0.007117</td>\n",
       "      <td>-0.013548</td>\n",
       "      <td>0.023085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0    -0.059661  0.018450 -0.039758 -0.042060 -0.044216 -0.023241 -0.027259   \n",
       "1    -0.035196  0.053715 -0.027906 -0.034745 -0.027877  0.007694  0.030240   \n",
       "2    -0.035196  0.053715 -0.027906 -0.034745 -0.027877  0.007694  0.030240   \n",
       "3    -0.017894  0.003590 -0.025652 -0.017793 -0.038787 -0.001943 -0.005159   \n",
       "4    -0.035196  0.053715 -0.027906 -0.034745 -0.027877  0.007694  0.030240   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995 -0.035196  0.053715 -0.027906 -0.034745 -0.027877  0.007694  0.030240   \n",
       "9996  0.009110  0.017185 -0.054455 -0.019663 -0.018070 -0.056120  0.045075   \n",
       "9997 -0.035196  0.053715 -0.027906 -0.034745 -0.027877  0.007694  0.030240   \n",
       "9998 -0.002061  0.022459 -0.043841  0.016776 -0.048276 -0.044551  0.012426   \n",
       "9999 -0.035196  0.053715 -0.027906 -0.034745 -0.027877  0.007694  0.030240   \n",
       "\n",
       "          7         8         9     ...      1014      1015      1016  \\\n",
       "0     0.020452  0.009933 -0.040778  ... -0.046408 -0.009997  0.039415   \n",
       "1     0.019474 -0.003791 -0.023077  ...  0.019346 -0.007953 -0.012474   \n",
       "2     0.019474 -0.003791 -0.023077  ...  0.019346 -0.007953 -0.012474   \n",
       "3    -0.006457 -0.000102 -0.027378  ...  0.028466 -0.019472  0.076911   \n",
       "4     0.019474 -0.003791 -0.023077  ...  0.019346 -0.007953 -0.012474   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.019474 -0.003791 -0.023077  ...  0.019346 -0.007953 -0.012474   \n",
       "9996 -0.065330 -0.018213 -0.047228  ...  0.009930 -0.037306  0.032301   \n",
       "9997  0.019474 -0.003791 -0.023077  ...  0.019346 -0.007952 -0.012474   \n",
       "9998  0.045399  0.004209 -0.018274  ... -0.002792  0.001135 -0.072303   \n",
       "9999  0.019474 -0.003790 -0.023077  ...  0.019346 -0.007953 -0.012474   \n",
       "\n",
       "          1017      1018      1019      1020      1021      1022      1023  \n",
       "0     0.026511  0.015673  0.006259  0.071344  0.019780 -0.042125  0.038383  \n",
       "1    -0.017535  0.005117 -0.003466 -0.002747 -0.007117 -0.013548  0.023085  \n",
       "2    -0.017535  0.005117 -0.003466 -0.002747 -0.007117 -0.013548  0.023085  \n",
       "3    -0.016450  0.009858 -0.015103  0.005643 -0.005126  0.005123  0.017724  \n",
       "4    -0.017535  0.005117 -0.003466 -0.002747 -0.007117 -0.013548  0.023085  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995 -0.017535  0.005117 -0.003466 -0.002747 -0.007117 -0.013548  0.023085  \n",
       "9996  0.038416 -0.035205  0.022948  0.050371 -0.011738 -0.034052  0.026258  \n",
       "9997 -0.017535  0.005117 -0.003466 -0.002747 -0.007117 -0.013548  0.023085  \n",
       "9998 -0.001034 -0.036395  0.012274  0.027492  0.010574 -0.050224 -0.019702  \n",
       "9999 -0.017535  0.005117 -0.003466 -0.002747 -0.007117 -0.013548  0.023085  \n",
       "\n",
       "[10000 rows x 1024 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(embeddings)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fee94d6-d3cd-49d1-9210-9c6c42bae6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name vonjack/bge-m3-gguf. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "vonjack/bge-m3-gguf does not appear to have a file named config.json. Checkout 'https://huggingface.co/vonjack/bge-m3-gguf/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-669e39ac-0a9a5f771094c8a6462d174a;a8b45f85-9d3b-4364-8625-853fcfebb0a3)\n\nEntry Not Found for url: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/config.json.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvonjack/bge-m3-gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model_broadcast \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mbroadcast(model)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine_similarity\u001b[39m(vec1, vec2):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:299\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    287\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    288\u001b[0m             model_name_or_path,\n\u001b[1;32m    289\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    312\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1324\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[1;32m   1322\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[0;32m-> 1324\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mset_base_model(model_name_or_path, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:53\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 53\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:965\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    963\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 965\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    967\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/utils/hub.py:456\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m         revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    461\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[0;31mOSError\u001b[0m: vonjack/bge-m3-gguf does not appear to have a file named config.json. Checkout 'https://huggingface.co/vonjack/bge-m3-gguf/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, monotonically_increasing_id\n",
    "from pyspark.sql.functions import col, lit, explode, posexplode, row_number, expr, collect_list\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "#model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"vonjack/bge-m3-gguf\", device=\"cpu\")\n",
    "\n",
    "model_broadcast = spark.sparkContext.broadcast(model)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    cos_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return float(cos_sim)\n",
    "\n",
    "def embedding(text):\n",
    "    model = model_broadcast.value\n",
    "    dense_vecs = model.encode(text, return_dense=True, max_length=4096)[\"dense_vecs\"]\n",
    "    return [float(x) for x in dense_vecs]\n",
    "\n",
    "embedding_udf = udf(embedding, ArrayType(DoubleType()))\n",
    "cosinesim_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "start=datetime.today()\n",
    "\n",
    "dfx = spark.createDataFrame([(string,) for string in s], [\"text\"])\n",
    "dfx = dfx.withColumn(\"rid\", monotonically_increasing_id())\n",
    "df = dfx.withColumn(\"vec\", embedding_udf(\"text\")).drop('text')\n",
    "\n",
    "df.cache()\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(f'\\n\\n{datetime.today()} - elapsed {datetime.today()-start}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508bea01-2aaf-4972-8c86-4cb9fdc76da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|rid2|                vec2|\n",
      "+----+--------------------+\n",
      "|   0|[-0.0596612505614...|\n",
      "|   1|[-0.0351958908140...|\n",
      "|   2|[-0.0351958908140...|\n",
      "|   3|[-0.0178942587226...|\n",
      "|   4|[-0.0351958908140...|\n",
      "|   5|[0.00349273602478...|\n",
      "|   6|[0.00826073810458...|\n",
      "|   7|[-0.0064715268090...|\n",
      "|   8|[-0.0351958908140...|\n",
      "|   9|[-0.0351958908140...|\n",
      "|  10|[-0.0351958908140...|\n",
      "|  11|[-0.0351958908140...|\n",
      "|  12|[-0.0351958908140...|\n",
      "|  13|[-0.0351958908140...|\n",
      "|  14|[-0.0109594613313...|\n",
      "|  15|[0.00754652032628...|\n",
      "|  16|[0.00774556165561...|\n",
      "|  17|[-0.0456585250794...|\n",
      "|  18|[-0.0351958908140...|\n",
      "|  19|[0.03130273148417...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+----+-------------------+\n",
      "|rid1|rid2|distance           |\n",
      "+----+----+-------------------+\n",
      "|0   |1   |0.6610146266968993 |\n",
      "|0   |2   |0.6610146266968993 |\n",
      "|0   |3   |0.5549171952369145 |\n",
      "|0   |4   |0.6610146266968993 |\n",
      "|0   |5   |0.4842509179948584 |\n",
      "|0   |6   |0.45500587648684154|\n",
      "|0   |7   |0.45938203846796577|\n",
      "|0   |8   |0.6610146266968993 |\n",
      "|0   |9   |0.6610146266968993 |\n",
      "|0   |10  |0.6610146266968993 |\n",
      "|0   |11  |0.6610146266968993 |\n",
      "|0   |12  |0.6610146266968993 |\n",
      "|0   |13  |0.6610146266968993 |\n",
      "|0   |14  |0.6394037165357032 |\n",
      "|0   |15  |0.49780857908228204|\n",
      "|0   |16  |0.4961267745218696 |\n",
      "|0   |17  |0.3150115249984541 |\n",
      "|0   |18  |0.6610146266968993 |\n",
      "|0   |19  |0.4753172886068574 |\n",
      "|0   |20  |0.6610146266968993 |\n",
      "+----+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "\n",
      "2024-07-20 18:22:11.761235 - elapsed 0:00:00.339462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start=datetime.today()\n",
    "\n",
    "df2 = df.withColumnRenamed(\"rid\", \"rid2\").withColumnRenamed(\"vec\", \"vec2\")\n",
    "\n",
    "df3 = df.join(df2, df.rid < df2.rid2).withColumn(\"distance\", 1.0 - cosinesim_udf(df.vec, df2.vec2)) \\\n",
    "        .select(col(\"rid\").alias('rid1'), \"rid2\", \"distance\")\n",
    "\n",
    "df3.show(truncate=False)\n",
    "\n",
    "print(f'\\n\\n{datetime.today()} - elapsed {datetime.today()-start}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fba687d-8bb4-4e06-9d9a-337f29d44f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df3.filter(df5.rid+2 == df5.rid2).show(truncate=False)\n",
    "#df3.filter(df3.distance<0.3).show(truncate=False)\n",
    "dfx.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76d92045-668b-4cf8-963c-5f713472ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/20 19:50:54 WARN TaskSetManager: Stage 89 contains a task of very large size (1358 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|rid |text                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2994|tout d'abord les repréciser à l'école. ensuite, interdire tout signe extérieur d'appartenance religieuse dans la rue, les lieux publics, les commerces                                                                                                                                                                                                                               |\n",
      "|5099|Appliquer la loi et interdire tout signe ostentatoire dans l'espace public (croix, foulard, hijab etc)                                                                                                                                                                                                                                                                               |\n",
      "|1508|Aucun signe religieux dans l'espace public. AUCUN ! Les nouveaux bâtiments religieux doivent etre neutres !! Plus aucune fête relieuse comme jour férié.                                                                                                                                                                                                                             |\n",
      "|638 |Hélas, interdire tout signe extérieur de religion dans le domaine public.                                                                                                                                                                                                                                                                                                            |\n",
      "|2606|faire strictement respecter la loi, n'autoriser aucun signe d'appartenance religieuse dans les espaces publics.                                                                                                                                                                                                                                                                      |\n",
      "|1639|interdire les signes religieux dans l'espace public et l'entreprise                                                                                                                                                                                                                                                                                                                  |\n",
      "|3460|interdire dans l'espace public , rues, administrations le port des signes extérieurs religieux (voile, kippa, croix)                                                                                                                                                                                                                                                                 |\n",
      "|4651|Aucun signe distinctif de religion dans les lieux publiques, entreprises, écoles, administrations, voie publique etc                                                                                                                                                                                                                                                                 |\n",
      "|713 |Interdire le port du voile ou tout autre représentation religieuse dans les lieux publiques: garderies, hôpitaux, établissements scolaires, universités, magasins et lieux culturels.  Seuls les gens de foi devraient être habilités à porter les signes distinctifs de la religion dont ils assurent la promotion. Interdire le prosélytisme provocateur, oppressif et envahissant.|\n",
      "|107 |Interdire les signes ostensibles dans tout l'espace public                                                                                                                                                                                                                                                                                                                           |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   1,   1, ...,   1,   0,   1],\n",
       "       ...,\n",
       "       [  0,   1,   1, ...,   1, 353,   1],\n",
       "       [  0,   1,   1, ...,   1, 353,   1],\n",
       "       [  0,   1,   1, ...,   1, 353,   1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.join(df3, (df3.rid1==0) & (df3.rid2==dfx.rid) & (df3.distance<0.3)).select('rid','text').show(10,truncate=False)\n",
    "\n",
    "import genieclust\n",
    "vecs = df.select('vec').rdd.map(lambda row: row['vec']).collect()\n",
    "vecs_array = np.array(vecs)\n",
    "\n",
    "k = genieclust.Genie(n_clusters=500, cast_float32=True, gini_threshold=0.2, affinity=\"cosinesimil\", \\\n",
    "                       exact=False, compute_all_cuts=True, compute_full_tree=True) \\\n",
    "          .fit_predict(vecs_array)\n",
    "print(k.shape)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675abb5-bae4-4201-b9a1-d4faf5920955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "\n",
    "# Convert distance matrix to DataFrame format suitable for Spark MLlib\n",
    "def create_distance_matrix_df(distance_matrix):\n",
    "    num_rows = len(distance_matrix)\n",
    "    rows = []\n",
    "    for i in range(num_rows):\n",
    "        row = Vectors.dense(distance_matrix[i])\n",
    "        rows.append((i, row))\n",
    "    return spark.createDataFrame(rows, [\"id\", \"features\"])\n",
    "\n",
    "# Convert the distance matrix into a DataFrame\n",
    "distance_df = create_distance_matrix_df(distance_matrix)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(k=5, seed=1)  # Adjust `k` as needed\n",
    "model = kmeans.fit(distance_df)\n",
    "predictions = model.transform(distance_df)\n",
    "\n",
    "# Show cluster centers and assignments\n",
    "print(\"Cluster Centers:\")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "print(\"Cluster Assignments:\")\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d793243-160e-4e37-a495-a0dd7c297ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taoufik/.local/lib/python3.11/site-packages/genieclust/genie.py:193: UserWarning: `compute_full_tree` is only available when `M` = 1 and `exact` is True\n",
      "  warnings.warn(\"`compute_full_tree` is only available when `M` = 1 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 10000)\n",
      "(10000, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   1,   1, ...,   1,   0,   1],\n",
       "       ...,\n",
       "       [  0,   1,   1, ...,   1, 164,   1],\n",
       "       [  0,   1,   1, ...,   1, 164,   1],\n",
       "       [  0,   1,   1, ...,   1, 164,   1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import genieclust\n",
    "\n",
    "# n_clusters=500\n",
    "embeddings1, embeddings2 = split(enbeddings) # how to split embeddings in 2\n",
    "k1 = genieclust.Genie(n_clusters=500, cast_float32=True, gini_threshold=0.2, affinity=\"cosinesimil\", \\\n",
    "                       exact=False, compute_all_cuts=True, compute_full_tree=True) \\\n",
    "          .fit_predict(embeddings1)\n",
    "k2 = genieclust.Genie(n_clusters=500, cast_float32=True, gini_threshold=0.2, affinity=\"cosinesimil\", \\\n",
    "                       exact=False, compute_all_cuts=True, compute_full_tree=True) \\\n",
    "          .fit_predict(embeddings2)\n",
    "k = merge(k1, k2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc266242-c767-4c1e-bd93-2d60011ae685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 40)\n",
      "(40, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 0, 2, 0, 1, 0, 1, 0,\n",
       "        0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 0, 1],\n",
       "       [0, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 0, 3, 0, 1, 0, 1, 0,\n",
       "        0, 3, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 0, 1],\n",
       "       [0, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 4, 3, 0, 1, 0, 1, 0,\n",
       "        0, 3, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 4, 1],\n",
       "       [0, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 4, 3, 0, 1, 0, 1, 0,\n",
       "        0, 3, 1, 2, 5, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 4, 1],\n",
       "       [0, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 4, 3, 0, 1, 0, 1, 0,\n",
       "        0, 3, 1, 2, 5, 3, 1, 1, 1, 1, 1, 1, 3, 6, 1, 3, 4, 1],\n",
       "       [0, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 4, 3, 0, 1, 0, 1, 0,\n",
       "        0, 3, 1, 2, 5, 3, 6, 1, 1, 1, 1, 1, 3, 7, 1, 3, 4, 1],\n",
       "       [0, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 4, 5, 3, 0, 1, 0, 1, 0,\n",
       "        0, 3, 1, 2, 6, 3, 7, 1, 1, 1, 1, 1, 3, 8, 1, 3, 5, 1],\n",
       "       [0, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 4, 5, 3, 0, 1, 0, 1, 0,\n",
       "        6, 3, 1, 2, 7, 3, 8, 1, 1, 1, 1, 1, 3, 9, 1, 3, 5, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import genieclust\n",
    "k = genieclust.Genie(n_clusters=10, cast_float32=True, gini_threshold=0.2, affinity=\"cosinesimil\", \\\n",
    "                       exact=False, compute_all_cuts=True, compute_full_tree=True) \\\n",
    "          .fit_predict(embeddings)\n",
    "print(k.shape)\n",
    "print(embeddings.shape)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea826e88-9e96-441d-b4a3-d06d3b41c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10000)\n",
      "(10000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k20</th>\n",
       "      <th>k50</th>\n",
       "      <th>k100</th>\n",
       "      <th>k200</th>\n",
       "      <th>k500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "      <td>117</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      k20  k50  k100  k200  k500\n",
       "0       0    0     0     0     0\n",
       "1       1    1     1     1     1\n",
       "2       1    1     1     1     1\n",
       "3       2    2     2     2     2\n",
       "4       1    1     1     1     1\n",
       "...   ...  ...   ...   ...   ...\n",
       "9995    1    1     1     1     1\n",
       "9996   14   21    48    57    68\n",
       "9997    1    1     1     1     1\n",
       "9998    3    5    79   117   360\n",
       "9999    1    1     1     1     1\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = k[[20, 50, 100, 200, 500], :]\n",
    "print(h.shape)\n",
    "h = np.transpose(h)\n",
    "print(h.shape)\n",
    "h = pd.DataFrame(h, columns=['k20', 'k50', 'k100', 'k200', 'k500'])\n",
    "#h = h.drop_duplicates() # why ?\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5163b8b6-faa5-4c86-9835-1eecd661d5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k2</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Pour utiliser des listes dans un dictionnaire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NMSLIB started as a personal project of Bilegs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pated in earlier evaluations. The most success...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>lude a modification of the VP-tree due to Boyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>z et al. (2013) and improved by David Novak, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k2                                            content\n",
       "0   0  Pour utiliser des listes dans un dictionnaire ...\n",
       "1   1  NMSLIB started as a personal project of Bilegs...\n",
       "2   2  pated in earlier evaluations. The most success...\n",
       "3   3  lude a modification of the VP-tree due to Boyt...\n",
       "4   4  z et al. (2013) and improved by David Novak, a..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dx2['k20'] = k20[50, :]\n",
    "dx2['k2'] = k20[5, :]\n",
    "\n",
    "s20 = dx2.groupby('k2').apply(lambda x: x.sample(n=7, replace=False) if len(x) >= 7 else x).reset_index(drop=True)\n",
    "grouped_df = s20.groupby('k2')['text'].apply(' . '.join).reset_index()\n",
    "grouped_df = grouped_df.rename(columns={'text': 'content'})\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1dcce-3e9e-4d11-bf2a-0ba6bbf3a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df):\n",
    "    from llama_cpp import Llama\n",
    "    llmc = Llama.from_pretrained(\n",
    "        repo_id=\"failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF\",\n",
    "        filename=\"Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\",\n",
    "        verbose=False, \n",
    "        n_ctx=4096,\n",
    "        #n_gpu_layers=-1, # move all to GPU\n",
    "        n_gpu_layers=0, # use CPU\n",
    "    )\n",
    "    content = df.iloc[0]['text']\n",
    "    prompt = f\"\"\"<|system|>You are a helpful assistant.<|end|>\n",
    "               <|user|>Summarize in french the following list of sentences in a short paragraph. Here is the list of sentence to summarize :{content}<|end|>\n",
    "               <|assistant|>\"\"\"\n",
    "    output = llmc(prompt, max_tokens=2048, stop=[\"<|endoftext|>\"])\n",
    "                # max_tokens=-1, echo=False, temperature=0.2, top_p=0.1)\n",
    "    return pd.DataFrame({'summary': [output['choices'][0]['text']]})\n",
    "\n",
    "# create summaries via Spark\n",
    "summaries = (df\n",
    "                .limit(1)\n",
    "                .groupby('content')\n",
    "                .applyInPandas(summarize, schema='summary string')\n",
    "                .show(vertical=True, truncate=False)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43d0deea-385f-489d-9f47-dd44c46658b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k2</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Pour créer un DataFrame pandas à partir d'un d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Il semble que nous ayons un texte en anglais q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k2                                        Description\n",
       "0   0  Pour créer un DataFrame pandas à partir d'un d...\n",
       "1   1  Il semble que nous ayons un texte en anglais q..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF\",\n",
    "    filename=\"Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\",\n",
    "    verbose=False, \n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=-1,\n",
    ")\n",
    "\n",
    "def process_content(content):\n",
    "    prompt = f\"\"\"<|system|>You are a helpful assistant.<|end|>\n",
    "               <|user|>Summarize in french the following list of sentences in a short paragraph. Here is the list of sentence to summarize :{content}<|end|>\n",
    "               <|assistant|>\"\"\"\n",
    "\n",
    "    output = llm(prompt, max_tokens=2048, stop=[\"<|endoftext|>\"])\n",
    "    res = output[\"choices\"][0][\"text\"].strip()\n",
    "    return res\n",
    "\n",
    "results = []\n",
    "for content in tqdm(grouped_df['content'], desc=\"Summarizing\"):  \n",
    "    summary = process_content(content)\n",
    "    results.append(summary)\n",
    "\n",
    "dz = pd.DataFrame(results, columns=['Description'])\n",
    "\n",
    "dz['k5'] = dz.index\n",
    "\n",
    "m1 = pd.merge(h, dz, on='k5')\n",
    "m1 = m1[['k2','k5','Description']]\n",
    "m1 = m1.drop_duplicates()\n",
    "\n",
    "gdf1 = m1.groupby('k2')['Description'].apply(' . '.join).reset_index()\n",
    "gdf1 = gdf1.rename(columns={'text': 'content'})\n",
    "gdf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43135abe-6e8a-44d9-8173-830a71701b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|k2 |Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |Pour créer un DataFrame pandas à partir d'un dictionnaire contenant des listes, il faut construire ce dernier puis convertir cet objet en DataFrame en utilisant la méthode pd.DataFrame(). Il est possible de définir les noms des colonnes pour le nouveau DataFrame via l'argument columns lors de cette conversion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|1  |Il semble que nous ayons un texte en anglais qui doit être traduit et résumé en français. Cependant, vous avez seulement fourni une portion du texte à traduire et résumer, sans terminer la liste de phrases fournies. Je vais donc procéder avec le segment donné:\\n\\n\\n\"NMSLIB a commencé comme un projet personnel de Bilegsaikhan Naidan qui a créé le code de base initial, les liens Python et...\"\\n\\n\\nTraduit et résumé en français (sujet à révision car la traduction est basique) :\\n\\nNMSLIB s'est initialement développé sous l’initiative personnelle de Bilegsaikhan Naidan, qui a élaboré le fondation logiciel et les liens Python. . Dans les évaluations antérieures, certaines méthodes ont été notées. La classe la plus réussie de méthodes--graphe proche/proximité--est représentée par le Hierarchical Navigable Small World Graph (HNSW). Cela est dû aux travaux de Malkov et Yashunin, en particulier les publications mentionnées ci-dessous. D'autres méthodes utiles existent également. . Dans l'ensemble de ces informations, il y a une proposition de modification du VP-tree, initiée par Boytsov et Naiden en 2013. En outre, il existe un indice de proximité APPROXIMATION (NAPP) proposé par Telle. Il faut noter que ces éléments se rapportent à des avancées techniques spécifiques dans le domaine de la linguistique computationnelle ou du traitement automatique du langage naturel, mais sans plus de contexte, une description détaillée est limitée. . Les travaux cités par z et al. en 2013 ont été améliorés par David Novak, ainsi qu'un index inversé non compressé. Cela implique que les recherches originales ont subi des modifications pour devenir plus performantes ou plus efficaces, avec la contribution spécifique de Novak et l'utilisation d'une méthode archives basique sans compression.|\n",
      "+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.createDataFrame(gdf1).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a6595-036d-4a8e-9cf0-99c35d12f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for content in tqdm(gdf1['Description'], desc=\"Summarizing\"):  \n",
    "    summary = process_content(content)\n",
    "    results.append(summary)\n",
    "\n",
    "dz2 = pd.DataFrame(results, columns=['Description'])\n",
    "dz.to_csv('dz1.csv', index=False)\n",
    "dz2.to_csv('dz2.csv', index=False)\n",
    "h.to_csv('h.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0eef7d-8d41-448b-acb2-957e63517968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/home/me/Téléchargements/laicite_gd.csv', delimiter=\";\")\n",
    "df['source'] = df['screen name']\n",
    "dx = df[['text','source']]\n",
    "dx2 = dx.dropna()\n",
    "\n",
    "def split_string(string, max_length=4096):\n",
    "    return [string[i:i + max_length] for i in range(0, len(string), max_length)]\n",
    "\n",
    "dx2['text'] = dx2['text'].apply(split_string)\n",
    "dx2 = dx2.explode('text').reset_index(drop=True)\n",
    "\n",
    "sent = dx2['text'].tolist()\n",
    "s = [str(x) for x in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e34e64-7f4d-4a90-b887-f3c3ea9cb0e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FlagEmbedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFlagEmbedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BGEM3FlagModel\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BGEM3FlagModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAAI/bge-m3\u001b[39m\u001b[38;5;124m'\u001b[39m, use_fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FlagEmbedding'"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "df['source'] = df['screen name']\n",
    "dx = df[['text','source']]\n",
    "dx2 = dx.dropna()\n",
    "\n",
    "def split_string(string, max_length=4096):\n",
    "    return [string[i:i + max_length] for i in range(0, len(string), max_length)]\n",
    "\n",
    "dx2['text'] = dx2['text'].apply(split_string)\n",
    "dx2 = dx2.explode('text').reset_index(drop=True)\n",
    "sent = dx2['text'].tolist()\n",
    "s = [str(x) for x in sent]\n",
    "\n",
    "embeddings = model.encode(s, return_dense=True,max_length=4096)\n",
    "embeddings = embeddings[\"dense_vecs\"]\n",
    "df1 = pd.DataFrame(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e5a73a4-b551-4446-8fff-c7941232e81d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'genieclust'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgenieclust\u001b[39;00m\n\u001b[1;32m      3\u001b[0m k20 \u001b[38;5;241m=\u001b[39m genieclust\u001b[38;5;241m.\u001b[39mGenie(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, cast_float32\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gini_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, affinity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosinesimil\u001b[39m\u001b[38;5;124m\"\u001b[39m, exact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, compute_all_cuts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, compute_full_tree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit_predict(embeddings)\n\u001b[1;32m      6\u001b[0m h \u001b[38;5;241m=\u001b[39m k20[[\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m], :]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'genieclust'"
     ]
    }
   ],
   "source": [
    "import genieclust\n",
    "\n",
    "k20 = genieclust.Genie(n_clusters=500, cast_float32=True, gini_threshold=0.2, affinity=\"cosinesimil\", exact=False, compute_all_cuts=True, compute_full_tree=True).fit_predict(embeddings)\n",
    "\n",
    "\n",
    "h = k20[[20, 50, 100, 200, 500], :]\n",
    "h = np.transpose(h)\n",
    "h = pd.DataFrame(h, columns=['k20', 'k50', 'k100', 'k200', 'k500'])\n",
    "h = h.drop_duplicates()\n",
    "\n",
    "\n",
    "dx2['k20'] = k20[50, :]\n",
    "\n",
    "s20 = dx2.groupby('k20').apply(lambda x: x.sample(n=7, replace=False) if len(x) >= 7 else x).reset_index(drop=True)\n",
    "grouped_df = s20.groupby('k20')['text'].apply(' . '.join).reset_index()\n",
    "grouped_df = grouped_df.rename(columns={'text': 'content'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f137eb-851f-4f57-a350-6c41a3f81065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF\",\n",
    "    filename=\"Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\",\n",
    "    verbose=False, \n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=-1,\n",
    ")\n",
    "\n",
    "def process_content(content):\n",
    "    prompt = f\"\"\"<|system|>You are a helpful assistant.<|end|>\n",
    "               <|user|>Summarize in french the following list of sentences in a short paragraph. Here is the list of sentence to summarize :{content}<|end|>\n",
    "               <|assistant|>\"\"\"\n",
    "\n",
    "    output = llm(prompt, max_tokens=2048, stop=[\"<|endoftext|>\"])\n",
    "    res = output[\"choices\"][0][\"text\"].strip()\n",
    "    return res\n",
    "\n",
    "results = []\n",
    "for content in tqdm(grouped_df['content'], desc=\"Summarizing\"):  \n",
    "    summary = process_content(content)\n",
    "    results.append(summary)\n",
    "\n",
    "dz = pd.DataFrame(results, columns=['Description'])\n",
    "\n",
    "dz['k50'] = dz.index\n",
    "\n",
    "m1 = pd.merge(h, dz, on='k50')\n",
    "m1 = m1[['k20','k50','Description']]\n",
    "m1 = m1.drop_duplicates()\n",
    "\n",
    "gdf1 = m1.groupby('k20')['Description'].apply(' . '.join).reset_index()\n",
    "gdf1 = gdf1.rename(columns={'text': 'content'})\n",
    "\n",
    "\n",
    "results = []\n",
    "for content in tqdm(gdf1['Description'], desc=\"Summarizing\"):  \n",
    "    summary = process_content(content)\n",
    "    results.append(summary)\n",
    "\n",
    "dz2 = pd.DataFrame(results, columns=['Description'])\n",
    "\n",
    "\n",
    "\n",
    "dz.to_csv('dz1.csv', index=False)\n",
    "dz2.to_csv('dz2.csv', index=False)\n",
    "h.to_csv('h.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
