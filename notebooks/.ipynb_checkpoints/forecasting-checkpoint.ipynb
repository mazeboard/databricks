{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97912581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %pip install -U sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../libs'))\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from init import spark\n",
    "from init import sc\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import initialize_context\n",
    "\n",
    "context = initialize_context()\n",
    "\n",
    "(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from workspace import Workspace\n",
    "from dataproject import DataProject\n",
    "#workspace = Workspace(context[\"workspace_id\"])\n",
    "data_project = DataProject(context[\"project_id\"])\n",
    "configuration = data_project.getConfiguration()\n",
    "\n",
    "content_clusters = [20, 60, 180, 500]\n",
    "\n",
    "(content_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, lit, to_number\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import spark_partition_id, col\n",
    "from pyspark.sql.types import *\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.utils import plotting\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.trend import STLForecaster\n",
    "from sktime.split import temporal_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61046a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_backward = 7 # Horizon sur lequel on veut tester le modèle vs le réalisé (réel connu)\n",
    "h_max = 7 # Horizon de prédiction (réel inconnu)\n",
    "k = 10 # Thème à prédire (prédiction univariée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO prepare data\n",
    "\n",
    "Data = pd.read_excel('Export_ts_daily.xlsx')\n",
    "\n",
    "data = spark.sql(f\"\"\"\n",
    "    SELECT ...\n",
    "\"\"\")\n",
    "\n",
    "data.limit(5).show(truncate=False)\n",
    "display(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f163a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On s'assure que la date est bien déclarée\n",
    "\n",
    "Data['time'] = pd.to_datetime(Data['time'], format='%Y%m%d')\n",
    "\n",
    "# On déclare la date en index (important pour sktime!)\n",
    "\n",
    "Data_ts = Data.set_index('time')\n",
    "\n",
    "# Pour cet exemple, on ne garde que les séries à partir de juin 2024 (trop de zéros avant).\n",
    "# On doit aussi choisir la classification à prédire\n",
    "\n",
    "Data_ts = Data_ts[(Data_ts.index>'2024-06-15') & (Data_ts['class']==k)]\n",
    "\n",
    "# On ne garde que l'index et la série modélisée (important pour sktime!)\n",
    "\n",
    "Data_ts = Data_ts.drop(['class'], axis=1)\n",
    "\n",
    "# Déclarer la fréquence des données (important pour sktime!)\n",
    "\n",
    "Data_ts.index = Data_ts.index.to_period(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8203ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(Data_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3983ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Définir l'horizon de prédiction\n",
    "\n",
    "fh = np.arange(1, h_max) \n",
    "\n",
    "# 2) Choisir le modèle de forecasting\n",
    "\n",
    "forecaster = AutoARIMA(sp = 7, suppress_warnings=True) \n",
    "\n",
    "# 3) Estimer le modèle sur les données d'entrainement\n",
    "\n",
    "forecaster.fit(Data_ts, fh=fh)\n",
    "\n",
    "# 4) Calculer la prévision\n",
    "\n",
    "y_pred = forecaster.predict(fh=fh) # Point forecasts\n",
    "y_pred_ints = forecaster.predict_interval(coverage=0.9) # Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4663540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also requires predictions\n",
    "y_pred = forecaster.predict()\n",
    "\n",
    "fig, ax = plotting.plot_series(\n",
    "    Data_ts, y_pred, \n",
    "    labels=[\"Data\", \"Forecast\"], \n",
    "    pred_interval=y_pred_ints\n",
    ")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avec lissage exponentiel\n",
    "\n",
    "forecaster = AutoETS(sp=7, auto = False, seasonal = \"add\")  \n",
    "forecaster.fit(Data_ts)  \n",
    "y_pred = forecaster.predict(fh=fh) # Point forecasts\n",
    "y_pred_ints = forecaster.predict_interval(coverage=0.9) # Intervals\n",
    "\n",
    "fig, ax = plotting.plot_series(\n",
    "    Data_ts, y_pred, \n",
    "    labels=[\"Data\", \"Forecast\"],\n",
    "    pred_interval=y_pred_ints\n",
    ")\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, on va garder une partie de la série sur laquelle on teste le modèle. \n",
    "# Le modèle est entrainé sur une fraction de la série (train set) et testé sur le reste (test set)\n",
    "\n",
    "y_train, y_test = temporal_train_test_split(Data_ts, test_size=h_backward) # On split le jeu de données\n",
    "plot_series(y_train, y_test, labels=[\"Train\", \"Test\"]) # On visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = ForecastingHorizon(y_test.index, is_relative=False) #Forecasting Horizon\n",
    "\n",
    "forecaster = AutoETS(sp=7, auto = False, seasonal = \"add\")  #Model\n",
    "\n",
    "y_pred = forecaster.fit(y_train, fh=fh).predict() # Forecast\n",
    "\n",
    "y_pred_ints = forecaster.predict_interval(coverage=0.9) # Intervals\n",
    "\n",
    "fig, ax = plotting.plot_series(\n",
    "    y_train, y_test, y_pred, \n",
    "    labels=[\"Data\", \"Test\",\"Forecast\"],\n",
    "    pred_interval=y_pred_ints\n",
    ")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel Forecasting\n",
    "\n",
    "# Un des gros intérêts de sktime est de pouvoir entrainer des modèles sur un panel de séries temporelles en un coup.\n",
    "\n",
    "# Dans notre cas, on va pouvoir entrainer des modèles sur les K thèmes de notre classification (10, 20, 50 ou plus)\n",
    "\n",
    "Data_panel = Data.copy()\n",
    "Data_panel = Data_panel[Data_panel.time>'2024-06-15'] # On ne garde que la période sur laquelle on a des valeurs\n",
    "Data_panel.set_index(['class', 'time'], inplace=True) # Double index\n",
    "Data_panel # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035469d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainer des modèles sur chaque hierarchie\n",
    "\n",
    "fh = np.arange(1, h_max) \n",
    "\n",
    "# Modèle AutoARIMA\n",
    "forecaster = AutoARIMA(sp = 7, suppress_warnings=True) \n",
    "\n",
    "# Prédiction\n",
    "y_pred = forecaster.fit(Data_panel, fh=fh).predict() #Point forecasts\n",
    "y_pred_ints = forecaster.predict_interval(coverage=0.9) # Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47343ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut représenter les forecast un-à-un en selectionnant une partie du tableau avec .xs\n",
    "\n",
    "class_to_plot = 14 # On choisit la catégorie à afficher\n",
    "\n",
    "fig, ax = plotting.plot_series(\n",
    "    Data_panel.xs(class_to_plot, level='class'), y_pred.xs(class_to_plot, level='class'), \n",
    "    labels=[\"Data\", \"Forecast\"],\n",
    "    pred_interval=y_pred_ints.xs(class_to_plot, level='class')\n",
    ")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split sur du Panel\n",
    "\n",
    "# Ici, on va entrainer un modèle sur toutes les classes k20 et on va prédire une partie des données observées\n",
    "\n",
    "y_train, y_test = temporal_train_test_split(Data_panel, test_size=h_backward) #On découpe le jeu entre train et test\n",
    "\n",
    "fh = np.arange(1, h_backward+1) # Forecasting Horizon\n",
    "\n",
    "forecaster = AutoETS(error = \"add\", trend = \"add\", seasonal = \"add\", sp=7)   # Modèle\n",
    "\n",
    "# Prédiction\n",
    "y_pred = forecaster.fit(y_train, fh=fh).predict() # Point forecasts\n",
    "y_pred_ints = forecaster.predict_interval(coverage=0.9) # Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efea16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_plot = 4 # On choisit la catégorie à afficher\n",
    "\n",
    "fig, ax = plotting.plot_series(\n",
    "    y_train.xs(class_to_plot, level='class'), y_test.xs(class_to_plot, level='class'), y_pred.xs(class_to_plot, level='class'),\n",
    "    labels=[\"Data\", \"Test (réel)\", \"Forecast\"],\n",
    "    pred_interval=y_pred_ints.xs(class_to_plot, level='class')\n",
    ")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf18ee-d4ca-48bb-9f08-79d9b238f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO generate forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM comp_forecasting\n",
    "    WHERE project_id='{context[\"project_id\"]}' AND\n",
    "          content_group='{context[\"import_id\"]}'\n",
    "\"\"\")\n",
    "\n",
    "topics = reduce(DataFrame.unionAll, [summary for content_cluster, summary in summaries.items()])\n",
    "\n",
    "topics.writeTo(f\"\"\"comp_forecasting\"\"\").using(\"delta\").append()\n",
    "\n",
    "display(topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
