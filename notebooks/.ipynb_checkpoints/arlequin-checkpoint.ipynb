{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d9f636-8d6e-459f-97e6-f1bbce1af037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------+-------------+\n",
      "|id |content                    |content_topic|\n",
      "+---+---------------------------+-------------+\n",
      "|1  |The cat is on the mat      |1            |\n",
      "|2  |My dog and cat are the best|1            |\n",
      "|3  |The locals are playing     |1            |\n",
      "+---+---------------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.20842222],\n",
       "       [0.20842266],\n",
       "       [1.16627871],\n",
       "       [0.208448  ],\n",
       "       [0.2084284 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit, explode, posexplode, row_number, expr\n",
    "from pyspark.sql.functions import sum, max, countDistinct\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, StopWordsRemover\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_top_n_terms_tfidf(tfidf_dtm, n=250):\n",
    "    term_scores = tfidf_dtm.sum(axis=0)\n",
    "    term_series = pd.Series(term_scores, index=tfidf_dtm.columns)\n",
    "    top_terms = term_series.sort_values(ascending=False).head(n)\n",
    "    return top_terms\n",
    "\n",
    "def extract_terms():\n",
    "    token_pattern = r'\\b\\w{3,}\\b'\n",
    "    vectorizer = TfidfVectorizer(token_pattern=token_pattern)\n",
    "    global_tfidf = pd.DataFrame(vectorizer.fit_transform(contents.select(col(\"content\")).toPandas()[\"content\"]).todense(), columns=vectorizer.get_feature_names_out())\n",
    "    global_tfidf['content_topic'] = contents.toPandas()['content_topic']\n",
    "    display(global_tfidf)\n",
    "    tfidf_per_content_topic = { content_topic: tfidf_per_topic.drop(columns='content_topic') for content_topic, tfidf_per_topic in global_tfidf.groupby('content_topic') }\n",
    "    top_terms_per_content_topic = {content_topic: get_top_n_terms_tfidf(content_topic_tfidf) for content_topic, content_topic_tfidf in tfidf_per_content_topic.items()}\n",
    "\n",
    "    flat_top_terms_per_content_topic = pd.concat(top_terms_per_content_topic, axis=1)\n",
    "\n",
    "    flat_top_terms_per_content_topic.reset_index(inplace=True)\n",
    "    melted_top_terms_per_content_topic = pd.melt(flat_top_terms_per_content_topic, id_vars=['index'], var_name='content_topic', value_name='score')\n",
    "\n",
    "    return spark.createDataFrame(melted_top_terms_per_content_topic).select(\n",
    "        #lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "        #lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "        col(\"content_topic\").cast('INTEGER'),\n",
    "        col(\"index\").alias(\"term\"),\n",
    "        col(\"score\").cast(\"DOUBLE\")\n",
    "    ).where(\"score IS NOT NULL\")\n",
    "\n",
    "\"\"\"extract terms using spark\"\"\"\n",
    "def extract_terms_spark():\n",
    "    tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "    TokenData = tokenizer.transform(contents)\n",
    "    \n",
    "    def extract_indices_from_vector(vector):\n",
    "        return vector.indices.tolist()\n",
    "    \n",
    "    def extract_values_from_vector(vector):\n",
    "        return vector.values.tolist()\n",
    "    \n",
    "    def filter_words(words):\n",
    "        return [word for word in words if len(word) >= 3]\n",
    "    \n",
    "    extract_indices_udf = udf(extract_indices_from_vector, ArrayType(IntegerType()))\n",
    "    extract_values_udf = udf(extract_values_from_vector, ArrayType(DoubleType()))\n",
    "    filter_words_udf = udf(filter_words, ArrayType(StringType()))\n",
    "    \n",
    "    data = TokenData.withColumn(\"filtered_words\", filter_words_udf(\"words\")).drop(\"words\")\n",
    "    \n",
    "    cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"tfidf_count_vector\")\n",
    "    cvModel = cv.fit(data)\n",
    "    data = cvModel.transform(data).drop(\"filtered_words\")\n",
    "    \n",
    "    idf = IDF(inputCol=\"tfidf_count_vector\", outputCol=\"features_tfidf\")\n",
    "    idfModel = idf.fit(data)\n",
    "    data = idfModel.transform(data).drop(\"tfidf_count_vector\")\n",
    "\n",
    "    data = data.withColumn(\"values\", extract_values_udf(\"features_tfidf\")) \\\n",
    "               .withColumn(\"indices\", extract_indices_udf(\"features_tfidf\")) \\\n",
    "               .drop(\"features_tfidf\")\n",
    "    data.show(truncate=False)\n",
    "\n",
    "    print(cvModel.vocabulary)\n",
    "    \n",
    "    explodedIndices = data.select(\"id\",\"content\",\"content_topic\",posexplode(col(\"indices\")).alias(\"pos\",\"idx\"))\n",
    "    \n",
    "    explodedValues = data.select(\"id\",\"content\",\"content_topic\",posexplode(col(\"values\")).alias(\"pos\",\"value\"))\n",
    "    \n",
    "    data = explodedIndices.join(explodedValues, (explodedValues.id == explodedIndices.id) & \\\n",
    "        (explodedIndices.pos == explodedValues.pos)) \\\n",
    "        .select(explodedIndices.id, explodedIndices.content, explodedIndices.content_topic, \\\n",
    "                explodedIndices.idx.alias(\"index\"), explodedValues.value.alias(\"score\"))\n",
    "   \n",
    "    grouped_data = data.groupBy('content_topic', 'index').agg(sum('score').alias('score'))\n",
    "    window_spec = Window.partitionBy('content_topic').orderBy(col('score').desc())\n",
    "    ranked_data = grouped_data.withColumn('rank', row_number().over(window_spec)).filter(col('rank') <= 250).drop('rank')\n",
    "\n",
    "    vocab_df = spark.createDataFrame([(i, term) for i, term in enumerate(cvModel.vocabulary)], [\"index\", \"term\"])\n",
    "    return ranked_data.join(vocab_df, on=\"index\").drop('index').orderBy(['content_topic', 'score'], ascending=[True, False]) \\\n",
    "        .select(\n",
    "            #lit(context[\"project_id\"]).alias(\"project_id\"),\n",
    "            #lit(context[\"import_id\"]).alias(\"content_group\"),\n",
    "            col(\"content_topic\").cast('INTEGER'),\n",
    "            col(\"term\"),\n",
    "            col(\"score\").cast(\"DOUBLE\")\n",
    "        ).where(\"score IS NOT NULL\")\n",
    "\n",
    "def minmax_tf():\n",
    "    tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "    TokenData = tokenizer.transform(contents)\n",
    "    \n",
    "    def extract_indices_from_vector(vector):\n",
    "        return vector.indices.tolist()\n",
    "    \n",
    "    def extract_values_from_vector(vector):\n",
    "        return vector.values.tolist()\n",
    "    \n",
    "    def filter_words(words):\n",
    "        return [word for word in words if len(word) >= 3]\n",
    "    \n",
    "    extract_indices_udf = udf(extract_indices_from_vector, ArrayType(IntegerType()))\n",
    "    extract_values_udf = udf(extract_values_from_vector, ArrayType(DoubleType()))\n",
    "    filter_words_udf = udf(filter_words, ArrayType(StringType()))\n",
    "    \n",
    "    data = TokenData.withColumn(\"filtered_words\", filter_words_udf(\"words\")).drop(\"words\")\n",
    "\n",
    "    data.show(truncate=False)\n",
    "    # Remove stop words\n",
    "    remover = StopWordsRemover(inputCol=\"filtered_words\", outputCol=\"terms\", stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "    data = remover.transform(data).drop('filtered_words')\n",
    "    data.show(truncate=False)\n",
    "\n",
    "    exploded = data.withColumn(\"term\", explode(col(\"terms\")))\n",
    "    term_doc_count = exploded.select(\"term\", \"id\").distinct() \\\n",
    "                         .groupBy(\"term\") \\\n",
    "                         .agg(countDistinct(\"id\").alias(\"doc_count\"))\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"terms\", outputCol=\"tfidf_count_vector\")\n",
    "    cvModel = cv.fit(data)\n",
    "    data = cvModel.transform(data) \\\n",
    "               .withColumn(\"values\", extract_values_udf(\"tfidf_count_vector\")) \\\n",
    "               .withColumn(\"indices\", extract_indices_udf(\"tfidf_count_vector\")) \\\n",
    "               .drop(\"features_tfidf\") \\\n",
    "               .drop('content') \\\n",
    "               .withColumn(\"nterms\",expr(\"aggregate(values, 0D, (acc, x) -> acc + x)\"))\n",
    "    data.show(truncate=False)\n",
    "\n",
    "    explodedIndices = data.select(\"id\",\"content_topic\",\"nterms\",posexplode(col(\"indices\")).alias(\"pos\",\"idx\"))\n",
    "    \n",
    "    explodedValues = data.select(\"id\",posexplode(col(\"values\")).alias(\"pos\",\"value\"))\n",
    "    \n",
    "    data = explodedIndices.join(explodedValues, (explodedValues.id == explodedIndices.id) & \\\n",
    "        (explodedIndices.pos == explodedValues.pos)) \\\n",
    "        .select(explodedIndices.content_topic, explodedIndices.idx.alias(\"index\"), \\\n",
    "                (explodedValues.value / explodedIndices.nterms).alias(\"tf\")) \\\n",
    "        .groupBy('content_topic', 'index').agg(max('tf').alias('max_tf'))\n",
    "    vocab_df = spark.createDataFrame([(i, term) for i, term in enumerate(cvModel.vocabulary)], [\"index\", \"term\"])\n",
    "    window_spec = Window.partitionBy('content_topic').orderBy(col('content_topic').asc(),col('max_tf').asc(),col('doc_count').asc())\n",
    "    data=data.join(vocab_df, on=\"index\").drop('index').join(term_doc_count, on=\"term\") \\\n",
    "            .withColumn('rank', row_number().over(window_spec)).filter(col('rank') <= 30).drop('rank')\n",
    "    data.show(truncate=False)\n",
    "\n",
    "def maxtf():\n",
    "    tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "    TokenData = tokenizer.transform(contents)\n",
    "    \n",
    "    def extract_indices_from_vector(vector):\n",
    "        return vector.indices.tolist()\n",
    "    \n",
    "    def extract_values_from_vector(vector):\n",
    "        return vector.values.tolist()\n",
    "    \n",
    "    def filter_words(words):\n",
    "        return [word for word in words if len(word) >= 3]\n",
    "    \n",
    "    extract_indices_udf = udf(extract_indices_from_vector, ArrayType(IntegerType()))\n",
    "    extract_values_udf = udf(extract_values_from_vector, ArrayType(DoubleType()))\n",
    "    filter_words_udf = udf(filter_words, ArrayType(StringType()))\n",
    "    \n",
    "    data = TokenData.withColumn(\"terms\", filter_words_udf(\"words\")).drop(\"words\")\n",
    "\n",
    "    # Remove stop words (need to determine language of docs)\n",
    "    #remover = StopWordsRemover(inputCol=\"filtered_words\", outputCol=\"terms\", stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "    #data = remover.transform(data).drop(\"filtered_words\")\n",
    "\n",
    "    exploded = data.withColumn(\"term\", explode(col(\"terms\")))\n",
    "    term_doc_count = exploded.select(\"term\", \"id\").distinct() \\\n",
    "                         .groupBy(\"term\") \\\n",
    "                         .agg(countDistinct(\"id\").alias(\"doc_count\"))\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"terms\", outputCol=\"tfidf_count_vector\")\n",
    "    cvModel = cv.fit(data)\n",
    "    data = cvModel.transform(data) \\\n",
    "            .withColumn(\"values\", extract_values_udf(\"tfidf_count_vector\")) \\\n",
    "            .withColumn(\"indices\", extract_indices_udf(\"tfidf_count_vector\")) \\\n",
    "            .drop(\"features_tfidf\") \\\n",
    "            .drop('content') \\\n",
    "            .withColumn(\"nterms\",expr(\"aggregate(values, 0D, (acc, x) -> acc + x)\"))\n",
    "\n",
    "    explodedIndices = data.select(\"id\", \"content_topic\",\"nterms\", posexplode(col(\"indices\")).alias(\"pos\",\"idx\"))\n",
    "    \n",
    "    explodedValues = data.select(\"id\", posexplode(col(\"values\")).alias(\"pos\",\"value\"))\n",
    "    \n",
    "    data = explodedIndices.join(explodedValues, (explodedValues.id == explodedIndices.id) & \\\n",
    "        (explodedIndices.pos == explodedValues.pos)) \\\n",
    "        .select(explodedIndices.content_topic, explodedIndices.idx.alias(\"index\"), \\\n",
    "                (explodedValues.value / explodedIndices.nterms).alias(\"tf\")) \\\n",
    "        .groupBy('content_topic', 'index').agg(max('tf').alias('max_tf'))\n",
    "    vocab_df = spark.createDataFrame([(i, term) for i, term in enumerate(cvModel.vocabulary)], [\"index\", \"term\"])\n",
    "    window_spec = Window.partitionBy('content_topic').orderBy(col('content_topic').asc(), col('score').desc())\n",
    "    data = data.join(vocab_df, on=\"index\").drop('index').join(term_doc_count, on=\"term\") \\\n",
    "                .select('content_topic', 'term', (col('max_tf') / col('doc_count')).alias('score')) \\\n",
    "                .withColumn('rank', row_number().over(window_spec)).filter(col('rank') <= 100).drop('rank')\n",
    "    data.show(truncate=False)\n",
    "\n",
    "#contents = spark.read.csv(\"/home/taoufik/Downloads/export.csv\", header=True)\n",
    "\n",
    "# Sample data\n",
    "contents = spark.createDataFrame([\n",
    "        Row(id=1, content=\"The cat is on the mat\", content_topic=1),\n",
    "        Row(id=2, content=\"My dog and cat are the best\", content_topic=1),\n",
    "        Row(id=3, content=\"The locals are playing\", content_topic=1)\n",
    "    ])\n",
    "contents.show(truncate=False)\n",
    "\n",
    "# Sample code for topic modeling using LDA\n",
    "vectorizer = CV(max_df=1.0, min_df=1.0, stop_words='english') #CV(max_df=0.9, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(contents.select(\"content\").toPandas())\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "topics = lda.components_\n",
    "topics\n",
    "#extract_terms().show()\n",
    "#extract_terms_spark().show()\n",
    "#minmax_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4518a-d2b9-47f0-846d-b68f78c221fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95433ad-3e79-4804-9e11-5b3c2c4e88a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
