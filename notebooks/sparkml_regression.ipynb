{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7f16cc-5039-4f72-9c8a-b4cadc8c5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7720378-0b03-42e3-b451-0f35fca13a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 19:11:27 WARN Utils: Your hostname, Ankbot resolves to a loopback address: 127.0.1.1; using 192.168.1.140 instead (on interface wlo1)\n",
      "24/08/26 19:11:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/26 19:11:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkML ARIMA Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06792c5b-bacd-4793-9894-002e52939ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "df = spark.read.csv(\"timeseries_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert the timestamp column to a proper timestamp data type\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Sort data by timestamp\n",
    "df = df.orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5dd6c30-b5a8-4629-8881-42a7ef44351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          timestamp|value|\n",
      "+-------------------+-----+\n",
      "|2023-01-01 00:00:00|  100|\n",
      "|2023-01-01 01:00:00|  102|\n",
      "|2023-01-01 02:00:00|  101|\n",
      "|2023-01-01 03:00:00|  105|\n",
      "|2023-01-01 04:00:00|  107|\n",
      "|2023-01-01 05:00:00|  110|\n",
      "|2023-01-01 06:00:00|  108|\n",
      "|2023-01-01 07:00:00|  111|\n",
      "|2023-01-01 08:00:00|  115|\n",
      "|2023-01-01 09:00:00|  120|\n",
      "|2023-01-01 10:00:00|  118|\n",
      "|2023-01-01 11:00:00|  122|\n",
      "|2023-01-01 12:00:00|  125|\n",
      "|2023-01-01 13:00:00|  130|\n",
      "|2023-01-01 14:00:00|  128|\n",
      "|2023-01-01 15:00:00|  133|\n",
      "|2023-01-01 16:00:00|  135|\n",
      "|2023-01-01 17:00:00|  138|\n",
      "|2023-01-01 18:00:00|  140|\n",
      "|2023-01-01 19:00:00|  142|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a14e233e-d40b-488e-be4e-b9a834af5ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n",
      "|          timestamp|value|        prediction|\n",
      "+-------------------+-----+------------------+\n",
      "|2023-01-01 00:00:00|  100|100.00000000000016|\n",
      "|2023-01-01 01:00:00|  102|102.00000000000014|\n",
      "|2023-01-01 02:00:00|  101|101.00000000000014|\n",
      "|2023-01-01 03:00:00|  105|105.00000000000013|\n",
      "|2023-01-01 04:00:00|  107|107.00000000000011|\n",
      "|2023-01-01 05:00:00|  110|110.00000000000009|\n",
      "|2023-01-01 06:00:00|  108| 108.0000000000001|\n",
      "|2023-01-01 07:00:00|  111|111.00000000000009|\n",
      "|2023-01-01 08:00:00|  115|115.00000000000006|\n",
      "|2023-01-01 09:00:00|  120|120.00000000000003|\n",
      "|2023-01-01 10:00:00|  118|118.00000000000004|\n",
      "|2023-01-01 11:00:00|  122|122.00000000000001|\n",
      "|2023-01-01 12:00:00|  125|124.99999999999999|\n",
      "|2023-01-01 13:00:00|  130|129.99999999999997|\n",
      "|2023-01-01 14:00:00|  128|127.99999999999997|\n",
      "|2023-01-01 15:00:00|  133|132.99999999999994|\n",
      "|2023-01-01 16:00:00|  135|134.99999999999994|\n",
      "|2023-01-01 17:00:00|  138|137.99999999999991|\n",
      "|2023-01-01 18:00:00|  140|139.99999999999991|\n",
      "|2023-01-01 19:00:00|  142| 141.9999999999999|\n",
      "+-------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 19:22:45 WARN Instrumentation: [eb9c4880] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Assemble features for SparkML (if you want to use regression or other ML models)\n",
    "assembler = VectorAssembler(inputCols=[\"value\"], outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Example of using a linear regression model in SparkML\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"value\")\n",
    "lr_model = lr.fit(df_assembled)\n",
    "predictions = lr_model.transform(df_assembled)\n",
    "\n",
    "predictions.select(\"timestamp\", \"value\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e3f8d97-7c98-4ec1-a061-4032a8cf91b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------+\n",
      "|          timestamp|value|features|\n",
      "+-------------------+-----+--------+\n",
      "|2023-01-01 00:00:00|  100| [100.0]|\n",
      "|2023-01-01 01:00:00|  102| [102.0]|\n",
      "|2023-01-01 02:00:00|  101| [101.0]|\n",
      "|2023-01-01 03:00:00|  105| [105.0]|\n",
      "|2023-01-01 04:00:00|  107| [107.0]|\n",
      "|2023-01-01 05:00:00|  110| [110.0]|\n",
      "|2023-01-01 06:00:00|  108| [108.0]|\n",
      "|2023-01-01 07:00:00|  111| [111.0]|\n",
      "|2023-01-01 08:00:00|  115| [115.0]|\n",
      "|2023-01-01 09:00:00|  120| [120.0]|\n",
      "|2023-01-01 10:00:00|  118| [118.0]|\n",
      "|2023-01-01 11:00:00|  122| [122.0]|\n",
      "|2023-01-01 12:00:00|  125| [125.0]|\n",
      "|2023-01-01 13:00:00|  130| [130.0]|\n",
      "|2023-01-01 14:00:00|  128| [128.0]|\n",
      "|2023-01-01 15:00:00|  133| [133.0]|\n",
      "|2023-01-01 16:00:00|  135| [135.0]|\n",
      "|2023-01-01 17:00:00|  138| [138.0]|\n",
      "|2023-01-01 18:00:00|  140| [140.0]|\n",
      "|2023-01-01 19:00:00|  142| [142.0]|\n",
      "+-------------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_assembled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cec14e01-7bf5-4b62-aadb-cd6059953cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          timestamp|value|\n",
      "+-------------------+-----+\n",
      "|2023-01-01 00:00:00|  100|\n",
      "|2023-01-01 01:00:00|  102|\n",
      "|2023-01-01 02:00:00|  101|\n",
      "|2023-01-01 03:00:00|  105|\n",
      "|2023-01-01 04:00:00|  107|\n",
      "|2023-01-01 05:00:00|  110|\n",
      "|2023-01-01 06:00:00|  108|\n",
      "|2023-01-01 07:00:00|  111|\n",
      "|2023-01-01 08:00:00|  115|\n",
      "|2023-01-01 09:00:00|  120|\n",
      "|2023-01-01 10:00:00|  118|\n",
      "|2023-01-01 11:00:00|  122|\n",
      "|2023-01-01 12:00:00|  125|\n",
      "|2023-01-01 13:00:00|  130|\n",
      "|2023-01-01 14:00:00|  128|\n",
      "|2023-01-01 15:00:00|  133|\n",
      "|2023-01-01 16:00:00|  135|\n",
      "|2023-01-01 17:00:00|  138|\n",
      "|2023-01-01 18:00:00|  140|\n",
      "|2023-01-01 19:00:00|  142|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eda7bd2d-6b2a-41d3-a310-8a90dc3dba4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 01:00:00    161.014818\n",
      "2023-01-02 02:00:00    163.260033\n",
      "2023-01-02 03:00:00    167.620581\n",
      "2023-01-02 04:00:00    170.774071\n",
      "2023-01-02 05:00:00    173.716787\n",
      "2023-01-02 06:00:00    176.156320\n",
      "2023-01-02 07:00:00    180.046492\n",
      "2023-01-02 08:00:00    183.247630\n",
      "2023-01-02 09:00:00    186.148445\n",
      "2023-01-02 10:00:00    188.703396\n",
      "Freq: H, Name: predicted_mean, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taoufik/.local/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/taoufik/.local/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/taoufik/.local/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency H will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/taoufik/.local/lib/python3.11/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame for ARIMA\n",
    "data = df.collect()\n",
    "pdf = pd.DataFrame(data, columns=df.columns)\n",
    "#pdf = df.toPandas()\n",
    "\n",
    "# Set the timestamp as index\n",
    "pdf.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(pdf['value'], order=(5, 1, 0))  # Example order (p,d,q)\n",
    "arima_model = model.fit()\n",
    "\n",
    "# Make predictions\n",
    "forecast = arima_model.forecast(steps=10)  # Forecast the next 10 steps\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f945388-ad8a-42dd-922c-aa0123f119b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25    109.529605\n",
      "26    139.156622\n",
      "27    110.903015\n",
      "28    138.943936\n",
      "29    114.313223\n",
      "30    136.527726\n",
      "31    114.721689\n",
      "32    135.307438\n",
      "33    115.832929\n",
      "34    134.173056\n",
      "Name: predicted_mean, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taoufik/.local/lib/python3.11/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Example: Aggregating and then applying ARIMA\n",
    "df_resampled = df.groupBy(window(\"timestamp\", \"1 hour\")).agg({\"value\": \"avg\"})\n",
    "pdf_resampled = df_resampled.toPandas()\n",
    "\n",
    "# Fit ARIMA model on resampled data\n",
    "model = ARIMA(pdf_resampled['avg(value)'], order=(5, 1, 0))\n",
    "arima_model = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast_resampled = arima_model.forecast(steps=10)\n",
    "print(forecast_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
